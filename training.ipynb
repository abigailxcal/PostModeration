{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ec761811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "\n",
    "# save for later\n",
    "#from sklearn.decomposition import PCA\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.datasets import make_classification\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import classification_report\n",
    "\n",
    "#os.chdir(r\"C:\\Users\\raned\\Documents\\GitHub\\PostModeration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29653db5",
   "metadata": {},
   "source": [
    "This notebook will start off with preprocessing the two csv files to train different supervised learning models. \n",
    "- Removal of usernames, URLs, and special characters\n",
    "- Lowercasing text\n",
    "- Tokenization (nltk or spaCy): breaking text into smaller units \n",
    "- Stopword removal: remove common words that become index terms (\"and\", \"or\", \"the\", \"in\")\n",
    "- Lemmatization: reduces words to their base or dictionary form\n",
    "- TF-IDF vectorization for feature extraction: a technique that converts text data into numerical vectors, representing the importance of words in a document relative to a collection of documents, by combining term frequency with inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a025d07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0         count   hate_speech  offensive_language  \\\n",
      "count  24783.000000  24783.000000  24783.000000        24783.000000   \n",
      "mean   12681.192027      3.243473      0.280515            2.413711   \n",
      "std     7299.553863      0.883060      0.631851            1.399459   \n",
      "min        0.000000      3.000000      0.000000            0.000000   \n",
      "25%     6372.500000      3.000000      0.000000            2.000000   \n",
      "50%    12703.000000      3.000000      0.000000            3.000000   \n",
      "75%    18995.500000      3.000000      0.000000            3.000000   \n",
      "max    25296.000000      9.000000      7.000000            9.000000   \n",
      "\n",
      "            neither         class  \n",
      "count  24783.000000  24783.000000  \n",
      "mean       0.549247      1.110277  \n",
      "std        1.113299      0.462089  \n",
      "min        0.000000      0.000000  \n",
      "25%        0.000000      1.000000  \n",
      "50%        0.000000      1.000000  \n",
      "75%        0.000000      1.000000  \n",
      "max        9.000000      2.000000  \n",
      "(24783, 7)\n",
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24783 entries, 0 to 24782\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Unnamed: 0          24783 non-null  int64 \n",
      " 1   count               24783 non-null  int64 \n",
      " 2   hate_speech         24783 non-null  int64 \n",
      " 3   offensive_language  24783 non-null  int64 \n",
      " 4   neither             24783 non-null  int64 \n",
      " 5   class               24783 non-null  int64 \n",
      " 6   tweet               24783 non-null  object\n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"TrainingData/labeled_data.csv\")\n",
    "print(df.describe())\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112d42c",
   "metadata": {},
   "source": [
    "**count**: number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were\n",
    "\n",
    "**hate_speech**: number of CF users who judged the tweet to be hate speech\n",
    "\n",
    "**offensive_language**: number of CF users who judged the tweet to be offensive\n",
    "\n",
    "**neither**: number of CF users who judged the tweet to be neither offensive nor non-offensive\n",
    "\n",
    "**class**: class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7a023f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    !!! rt @mayasolovely: as a woman you shouldn't...\n",
       "1    !!!!! rt @mleew17: boy dats cold...tyga dwn ba...\n",
       "2    !!!!!!! rt @urkindofbrand dawg!!!! rt @80sbaby...\n",
       "3    !!!!!!!!! rt @c_g_anderson: @viva_based she lo...\n",
       "4    !!!!!!!!!!!!! rt @shenikaroberts: the shit you...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrubbing text: removing usernames, URLs, special characters and ensuring all text is lowercase\n",
    "tweet_column = df['tweet'].astype(str).str.casefold()  # lowercase\n",
    "tweet_column.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4b94924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     as a woman you shouldn t complain about clean...\n",
       "1     boy dats cold tyga dwn bad for cuffin dat hoe...\n",
       "2     dawg you ever fuck a bitch and she start to c...\n",
       "3                               she look like a tranny\n",
       "4     the shit you hear about me might be true or i...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removes usernames first, urls, then any special characters\n",
    "clean_tweet = tweet_column.str.replace(r'(rt)?\\s?@\\w+:?', ' ', regex=True).str.replace(r'http.+', ' ', regex=True).str.replace(r'\\W+', ' ', regex=True)\n",
    "clean_tweet.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b63046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [woman, complain, cleaning, house, amp, man, a...\n",
      "1    [boy, dat, cold, tyga, dwn, bad, cuffin, dat, ...\n",
      "2    [dawg, ever, fuck, bitch, start, cry, confused...\n",
      "3                                 [look, like, tranny]\n",
      "4    [shit, hear, might, true, might, faker, bitch,...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#tokenization, stop words, and lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "#nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) #stopwords\n",
    "\n",
    "def clean_tokenize(text): \n",
    "    tokens = word_tokenize(text)  # Keeps contractions like \"don't\"; tokenization\n",
    "    tokens = [t.lower() for t in tokens if t.isalpha() or \"'\" in t]  # keep letters + contractions\n",
    "    tokens = [t for t in tokens if t != \"rt\" and t not in stop_words]  # remove 'rt' and stopwords\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens] #lemmatization\n",
    "    return lemmatized\n",
    "\n",
    "cleaned_tokens = clean_tweet.apply(clean_tokenize)\n",
    "print(cleaned_tokens.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eab4ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaaaaaaaand' 'aaahhhhh' 'aahahah' 'aaliyah' 'aan' 'aap' 'aaron'\n",
      " 'aaronmacgruder' 'aaryn' 'ab' 'abandonado' 'abbey' 'abby' 'abc' 'abdelka'\n",
      " 'abduction' 'abdullah' 'abdurahman' 'abed' 'abel' 'aberdeen' 'ability'\n",
      " 'able' 'abo' 'aborted' 'abortion' 'abou' 'abound' 'abouta' 'abouttime'\n",
      " 'abraham' 'absent' 'absolute' 'absolutely' 'absoluteyvile' 'absolved'\n",
      " 'abstract' 'absurd' 'abt' 'abu' 'abundance' 'abus' 'abuse' 'abused'\n",
      " 'abuser' 'abusive' 'ac' 'aca' 'acab' 'academic' 'accelerated' 'accent'\n",
      " 'accept' 'acceptable' 'acceptance' 'accepted' 'access' 'accessible'\n",
      " 'accessorize' 'accessory' 'accident' 'accidentally' 'accipiter'\n",
      " 'accipitridae' 'accnt' 'accolade' 'accompanied' 'accord' 'according'\n",
      " 'accordingly' 'account' 'accountable' 'accountant' 'acct' 'accuracy'\n",
      " 'accurate' 'accurately' 'accused' 'accuses' 'accustomed' 'acdc' 'ace'\n",
      " 'aceptar' 'aceves' 'ach' 'achieve' 'achilles' 'aching' 'acid' 'ackin'\n",
      " 'acknowledge' 'acknowledged' 'acknowledging' 'acl' 'acne' 'acoustic'\n",
      " 'acquire' 'acre' 'acronym']\n",
      "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0               0      3            0                   0        3      2   \n",
      "1               1      3            0                   3        0      1   \n",
      "2               2      3            0                   3        0      1   \n",
      "3               3      3            0                   2        1      1   \n",
      "4               4      6            0                   6        0      1   \n",
      "...           ...    ...          ...                 ...      ...    ...   \n",
      "24778       25291      3            0                   2        1      1   \n",
      "24779       25292      3            0                   1        2      2   \n",
      "24780       25294      3            0                   3        0      1   \n",
      "24781       25295      6            0                   6        0      1   \n",
      "24782       25296      3            0                   0        3      2   \n",
      "\n",
      "                                                   tweet  \\\n",
      "0      !!! RT @mayasolovely: As a woman you shouldn't...   \n",
      "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
      "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
      "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
      "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
      "...                                                  ...   \n",
      "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...   \n",
      "24779  you've gone and broke the wrong heart baby, an...   \n",
      "24780  young buck wanna eat!!.. dat nigguh like I ain...   \n",
      "24781              youu got wild bitches tellin you lies   \n",
      "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...   \n",
      "\n",
      "                                            cleaned_text  \n",
      "0      woman complain cleaning house amp man always t...  \n",
      "1         boy dat cold tyga dwn bad cuffin dat hoe place  \n",
      "2           dawg ever fuck bitch start cry confused shit  \n",
      "3                                       look like tranny  \n",
      "4         shit hear might true might faker bitch told ya  \n",
      "...                                                  ...  \n",
      "24778  muthaf lie right tl trash mine bible scripture...  \n",
      "24779    gone broke wrong heart baby drove redneck crazy  \n",
      "24780  young buck wan na eat dat nigguh like aint fuc...  \n",
      "24781                     youu got wild bitch tellin lie  \n",
      "24782  ruffled ntac eileen dahlia beautiful color com...  \n",
      "\n",
      "[24783 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df1['cleaned_text'] = cleaned_tokens.apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df1['cleaned_text'])\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(feature_names[:100])  \n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0d630e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (27000, 33004)\n",
      "['aa' 'aaa' 'aaaa' 'aaaaa' 'aaaaaaaaaaaaaaaaa' 'aaaaaaacopyrighta'\n",
      " 'aaaaaaareaareaaaaaaaaaaaaaa' 'aaaaarrrrrggggghhhhh' 'aaaacg'\n",
      " 'aaadonaaat' 'aaah' 'aaand' 'aachen' 'aaeyou' 'aag' 'aah' 'aaib' 'aaj'\n",
      " 'aak' 'aalukkoru' 'aand' 'aanti' 'aap' 'aardvark' 'aaron' 'aaroncrick'\n",
      " 'aarp' 'aau' 'ab' 'aba' 'aback' 'abacus' 'abandon' 'abandoned' 'abash'\n",
      " 'abated' 'abaxial' 'abb' 'abba' 'abbey' 'abbott' 'abbreviated'\n",
      " 'abbreviation' 'abc' 'abd' 'abdf' 'abdomen' 'abduce' 'abdul' 'abdullah'\n",
      " 'abe' 'abecedary' 'abeh' 'abel' 'abelson' 'aberdeen' 'abet' 'abeyance'\n",
      " 'abf' 'abhishek' 'abhorrent' 'abidance' 'abide' 'abiding' 'abigail'\n",
      " 'ability' 'abiogenic' 'abject' 'abk' 'abkhazia' 'able' 'abm' 'abnegation'\n",
      " 'abner' 'abnormal' 'abnormality' 'aboard' 'abode' 'abolish' 'abolished'\n",
      " 'abolishment' 'abolitionist' 'abominable' 'abominably' 'abominate'\n",
      " 'abomination' 'aboridzinima' 'aboriginal' 'aborigine' 'abort' 'aborted'\n",
      " 'abortion' 'abortive' 'abortively' 'abound' 'abp' 'abraham' 'abrahamic'\n",
      " 'abramowicz' 'abrams']\n"
     ]
    }
   ],
   "source": [
    "#PREPROCESSING FOR HateSpeechDatasetBalanced.csv\n",
    "\n",
    "#Load dataset and take a 27,000-row sample; easier to have random 27,000 samples due to how big the actual dataset is \n",
    "df = pd.read_csv(\"TrainingData/HateSpeechDatasetBalanced.csv\")\n",
    "df_subset = df.sample(n=27000, random_state=42).copy()\n",
    "\n",
    "df_subset['Content'] = df_subset['Content'].astype(str).str.casefold()\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    tokens = word_tokenize(text)  # splits into words and keeps contractions\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # keep only alphabetic tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "df_subset['cleaned_tokens'] = df_subset['Content'].apply(clean_tokenize)\n",
    "df_subset['cleaned_text'] = df_subset['cleaned_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "df_subset['cleaned_text'] = df_subset['cleaned_text'].fillna('')\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df_subset['cleaned_text'])  # Features\n",
    "y = df_subset['Label']  # Target labels\n",
    "\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "print(tfidf.get_feature_names_out()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1547d2",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae742d90",
   "metadata": {},
   "source": [
    "## Start Here\n",
    "The code below combines the two datasets into one dataframe before any thing gets preprocessed. Since SVMs and LSTMs require different formats for their text input data, I created new columns 'tokenized_clean_text' and 'cleaned_text' that contain different formats of text so that training_data_df['Content'] doesn't have to be re-preprocessed all over again wheneveer we switch between models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "ecbe47ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import LinearSVC  # for text, linear SVM usually works best\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# LSTM\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras._tf_keras.keras.models import Sequential\n",
    "from keras._tf_keras.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "#nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ea029",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# these two variables should must always add up to 14,500 to ensure balance between hate speech vs. non hate speech \n",
    "\n",
    "emotion_data_size = 11500          #dictates how much of the training data comes from text.csv (emotion data)\n",
    "hateSpeechBalanced_size = 3000     #dictates how much of the training data comes from HateSpeechDatasetBalanced.csv \n",
    "\n",
    "# ====== RESULTS ===========\n",
    "#HateSpeechBalanced_size = 3000: SVM 90%, LSTM 89%\n",
    "#HateSpeechBalanced_size = 4300: SVM 88%, LSTM 87%\n",
    "#HateSpeechBalanced_size = 6300: SVM 87%, LSTM 84%\n",
    "#HateSpeechBalanced_size = 8300: SVM 85%, LSTM 82%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577dd945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11200, 2)\n",
      "Label\n",
      "0    11200\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ================== Additional Dataset: text.csv =======================\n",
    "emotions_df = pd.read_csv(\"TrainingData/text.csv\")\n",
    "emotions_df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "\n",
    "# most of the rows contain the word \"feeling/feels/feel\" which might skew the data, the code below will ensure the data is balanced in that respect\n",
    "no_feeling_size = int(emotion_data_size/2)\n",
    "feeling_size = int(no_feeling_size/5)\n",
    "\n",
    "feeling_df = emotions_df[emotions_df['text'].str.contains(r'\\bfeel(?:s|ing)?\\b', case=False, regex=True)]\n",
    "no_feeling_df= emotions_df[~emotions_df['text'].str.contains(r'\\bfeel(?:s|ing)?\\b', case=False, regex=True)].sample(n=no_feeling_size, random_state=42).copy()\n",
    "\n",
    "\n",
    "emotions = pd.DataFrame()\n",
    "for i in range(0,5):\n",
    "    temp = feeling_df[feeling_df['label']==i].sample(n=feeling_size, random_state=42).copy()\n",
    "    emotions = pd.concat([emotions,temp],ignore_index=True)\n",
    "\n",
    "\n",
    "emotions = pd.concat([emotions,no_feeling_df],ignore_index=True)\n",
    "\n",
    "emotions['Content'] = emotions['text']\n",
    "emotions['Label'] = emotions['label']\n",
    "emotions.drop(columns=['text','label'],inplace=True)\n",
    "emotions['Label']= emotions['Label'].replace(to_replace=[0,1,2,3,4,5], value = [0,0,0,0,0,0])\n",
    "print(emotions.shape)\n",
    "print(emotions['Label'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ff555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37000, 2)\n",
      "(37000, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# since both datasets have \"Content/Tweet\" and \"Label/class\" columns, the following code will merge the two datasets into one dataframe while maintaining balance\n",
    "\n",
    "# #============================================ labeled_data.csv ============================================\n",
    "df1 = pd.read_csv(\"TrainingData/labeled_data.csv\")\n",
    "#reduced_df1['class'].value_counts()    #1430 tweets marked at hate-speech, so we will extract 1430 marked for hate speech and 1430 that aren't\n",
    "\n",
    "# get rid of all the extra columns that aren't relevant\n",
    "reduced_df1 = df1[['tweet','class']]\n",
    "\n",
    "# extracts 1430 marked for hate speech and 1430 that aren't and combine into one df\n",
    "hatespeech = reduced_df1[reduced_df1['class']==0].sample(n=1430, random_state=42).copy() # hate speech\n",
    "nonHateful = reduced_df1[reduced_df1['class']==2].sample(n=1430, random_state=42).copy() # not hate speech\n",
    "sampled_hatespeech_df = pd.concat([hatespeech,nonHateful])\n",
    "\n",
    "\n",
    "# edit the values in 'Class' so that they match the values for HateSpeechDatasetBalanced.csv \n",
    "# Clean: 0, Hate speech: 1\n",
    "sampled_hatespeech_df['Content'] = sampled_hatespeech_df['tweet']\n",
    "sampled_hatespeech_df['Label'] = sampled_hatespeech_df['class'].replace(to_replace=[0,2], value = [1,0])\n",
    "sampled_hatespeech_df = sampled_hatespeech_df.drop(columns=['tweet','class'])\n",
    "sampled_hatespeech_df = pd.concat([sampled_hatespeech_df,emotions])\n",
    "\n",
    "\n",
    "#============================================ HateSpeechDatasetBalanced.csv ============================================\n",
    "df = pd.read_csv(\"TrainingData/HateSpeechDatasetBalanced.csv\")\n",
    "\n",
    "df_subset = df.sample(n=37000, random_state=42).copy()\n",
    "print(df_subset.shape)\n",
    "\n",
    "temp_df = df_subset[['Content','Label']]\n",
    "print(temp_df.shape)\n",
    "\n",
    "hatespeech = temp_df[temp_df['Label']==1].sample(n=14500, random_state=42).copy() # hate speech\n",
    "nonHateful = temp_df[temp_df['Label']==0].sample(n=hateSpeechBalanced_size, random_state=42).copy() # nonhate speech\n",
    "\n",
    "\n",
    "# combine both datasets into one:\n",
    "training_data_df = pd.concat([hatespeech,nonHateful],ignore_index=True)\n",
    "training_data_df = pd.concat([training_data_df,sampled_hatespeech_df],ignore_index=True)\n",
    "\n",
    "\n",
    "training_data_df['Content']= training_data_df['Content'].astype(str).str.casefold()\n",
    "clean_content = training_data_df['Content'].str.replace(r'(rt)?\\s?@\\w+:?', ' ', regex=True).str.replace(r'http.+', ' ', regex=True)\n",
    "training_data_df['Content'] = clean_content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "819254b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "ee69086e-506f-4479-93ea-614cb4556141",
       "rows": [
        [
         "0",
         "see this sums up your personal wiki and administrator of wiki class piece of shit even lower than that you as an administrator just violated civil and you remind of thousand wikipedia civil obedience blah blah blah well i am weeelllll are aware of all your policies and since none of you administrator cared to see that the article what a slut a complete fake and full of lies and instead of that accused me i also forgot to acknowledge even a bit of your policies why should i care about the policies when admins violate it themselves btw if even one of you had cared to look the article and its fake references and no references even the issue would have solved where are you ago this again sums up your class most of you admins or who even veteran admins are either illiterate or did not have a job to do retired and classes a bunch of arrogant idiots some be responsible and educated i know a few but most of you clowns don't even listen to them a clear proof me i have been watching tennis and following records for decades so once i saw that article i refused it going directly for a fan puffer proven also by the references it used like the google tennis warehouse fan chat room website some blogs etc btw i think i also am ahead in terms of academic degree than most half of you i wrote this long comment because i know instead of punishing myself the article creator and editing all its ridiculously funny claims about federer you would read this and come back and teach me blah blah wiki civil wiki etc you just wasted your time",
         "1"
        ],
        [
         "1",
         "the beach is calling weekend has already begun celebrate happyjohnmccainday trumpisatraitor votebluetosaveamerica",
         "1"
        ],
        [
         "2",
         "well you lord fuck this faggot",
         "1"
        ],
        [
         "3",
         "grave treachery is also the next most significant quality of southern goat ropers",
         "1"
        ],
        [
         "4",
         "bytch you voice stupid as f ck first off if those were my got damn kids note i t receive any kids but if i did d be calling my girlfriend up and letting her know that my kids would be referring to her as mammy shannon from now on and if the bitch didn t like it too mother skin high risk if her talentless ass didn t have to pay for d ck she wouldn t be having all these f skin issues in the first place but apparently her same sex game must be awfully wack because ve noticed that she never keeps any man s e'er been with interested in her for more than a few years then she either gets dumped or cheated on and truth be tell shannon the lima is times finer than j lo is and this whore thinks she can stand up and walk on stage and act as if her pussy just gleaming like the damn day she what a slut born are not no man going to touch that stinky fish ass sandwich no way",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>see this sums up your personal wiki and admini...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the beach is calling weekend has already begun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well you lord fuck this faggot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grave treachery is also the next most signific...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bytch you voice stupid as f ck first off if th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Label\n",
       "0  see this sums up your personal wiki and admini...      1\n",
       "1  the beach is calling weekend has already begun...      1\n",
       "2                     well you lord fuck this faggot      1\n",
       "3  grave treachery is also the next most signific...      1\n",
       "4  bytch you voice stupid as f ck first off if th...      1"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b236b",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "6332e4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Content  Label  \\\n",
      "0  see this sums up your personal wiki and admini...      1   \n",
      "1  the beach is calling weekend has already begun...      1   \n",
      "2                     well you lord fuck this faggot      1   \n",
      "3  grave treachery is also the next most signific...      1   \n",
      "4  bytch you voice stupid as f ck first off if th...      1   \n",
      "\n",
      "                                      cleaned_tokens  \\\n",
      "0  [see, sum, personal, wiki, administrator, wiki...   \n",
      "1  [beach, calling, weekend, already, begun, cele...   \n",
      "2                         [well, lord, fuck, faggot]   \n",
      "3  [grave, treachery, also, next, significant, qu...   \n",
      "4  [bytch, voice, stupid, f, ck, first, got, damn...   \n",
      "\n",
      "                              tokenized_cleaned_text  \n",
      "0  see sum personal wiki administrator wiki class...  \n",
      "1  beach calling weekend already begun celebrate ...  \n",
      "2                              well lord fuck faggot  \n",
      "3  grave treachery also next significant quality ...  \n",
      "4  bytch voice stupid f ck first got damn kid not...  \n",
      "TF-IDF shape: (31560, 311317)\n",
      "['aa' 'aa bfmv' 'aa blink' 'aa car' 'aa lol' 'aa meeting' 'aa miw'\n",
      " 'aa shit' 'aa smug' 'aaa' 'aaa know' 'aaa mean' 'aaa padding' 'aaa woo'\n",
      " 'aaaaa' 'aaaaa base' 'aaaaaaaaaaaaaaaaa' 'aaaaaaaaaaaaaaaaa obavezno'\n",
      " 'aaaaaaaaaaaaaaarrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrggggggggggggggggggggggggggggghhhhhhhhhhhhhh'\n",
      " 'aaaaaaaaand' 'aaaaaaaaand begin' 'aaaah' 'aaaah thank' 'aaadonaaat'\n",
      " 'aaadonaaat let' 'aaah' 'aaah gladstone' 'aachen' 'aachen daphne'\n",
      " 'aaeyou' 'aaeyou arrogant' 'aag' 'aag leigh' 'aahhh' 'aaj' 'aaj taxon'\n",
      " 'aand' 'aand back' 'aand starting' 'aanti' 'aanti islamista' 'aap'\n",
      " 'aap maoist' 'aap rule' 'aaron' 'aaron finding' 'aaron finishing'\n",
      " 'aaron impatient' 'aaron malawian' 'aaron perfect' 'aaron spelling'\n",
      " 'aaron swartz' 'aaron want' 'aaronmacgruder' 'aaronmacgruder stuff'\n",
      " 'aarp' 'aarp soviet' 'aatp' 'aatp angelic' 'aau' 'ab' 'ab din'\n",
      " 'ab initio' 'ab normal' 'ab requested' 'ab soul' 'ab zastupaju' 'aba'\n",
      " 'aba cure' 'aba cured' 'aba home' 'aback' 'aback feeling' 'abacus'\n",
      " 'abacus diet' 'abandon' 'abandon cursed' 'abandon feeling'\n",
      " 'abandon livejournal' 'abandoned' 'abandoned child' 'abandoned fiance'\n",
      " 'abandoned growing' 'abandoned judged' 'abandoned kid'\n",
      " 'abandoned military' 'abandoned next' 'abandoned plan'\n",
      " 'abandoned portrait' 'abandoning' 'abandoning idea' 'abandonment'\n",
      " 'abandonment inspect' 'abandonment jealousy' 'abandonment past' 'abb'\n",
      " 'abb agni' 'abba' 'abba dew' 'abbas']\n"
     ]
    }
   ],
   "source": [
    "# Cleaning for models that require sparse vectors as text  (SVMs,Logistic Regression, etc)\n",
    "\n",
    "#tokenization, stop words, and lemmatization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) #stopwords\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    tokens = word_tokenize(text)  # splits into words and keeps contractions\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # keep only alphabetic tokens\n",
    "    tokens = [t for t in tokens if t != \"rt\" and t not in stop_words]  # remove 'rt' and stopwords\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "training_data_df['cleaned_tokens'] = training_data_df['Content'].apply(clean_tokenize)\n",
    "training_data_df['tokenized_cleaned_text'] = training_data_df['cleaned_tokens'].apply(lambda x: ' '.join(x))\n",
    "training_data_df['tokenized_cleaned_text'] = training_data_df['tokenized_cleaned_text'].fillna('')\n",
    "\n",
    "print(training_data_df.head())\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2))      # playing around with ngram_range, might go up to 3\n",
    "\n",
    "X = tfidf.fit_transform(training_data_df['tokenized_cleaned_text'])  # Features\n",
    "y = training_data_df['Label']  # Target labels\n",
    "\n",
    "print(\"TF-IDF shape:\", X.shape)\n",
    "print(tfidf.get_feature_names_out()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "bd28bb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2633  461]\n",
      " [ 199 3019]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89      3094\n",
      "           1       0.87      0.94      0.90      3218\n",
      "\n",
      "    accuracy                           0.90      6312\n",
      "   macro avg       0.90      0.89      0.90      6312\n",
      "weighted avg       0.90      0.90      0.90      6312\n",
      "\n",
      "The tweet: ' stfu you piece of trash ' is permitted . Value:  0\n",
      "The tweet: ' Hope you're having a good day you cute piece of shit ' has been flagged for hate speech. . Value:  1\n",
      "The tweet: ' hello there ' has been flagged for hate speech. . Value:  1\n",
      "The tweet: ' today was fine ' has been flagged for hate speech. . Value:  1\n"
     ]
    }
   ],
   "source": [
    "# SVM training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LinearSVC(C=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "legend = {0: \"' is permitted\", 1: \"' has been flagged for hate speech.\"}\n",
    "\n",
    "\n",
    "def moderate(tweet):\n",
    "    tweet_clean = \" \".join(clean_tokenize(tweet))\n",
    "    tweet_vec = tfidf.transform([tweet_clean])\n",
    "    pred = model.predict(tweet_vec)[0]\n",
    "    print(\"The tweet: '\",tweet,legend[pred],\". Value: \",pred)\n",
    "    \n",
    "    \n",
    "moderate(\"stfu you piece of trash\")  # 1\n",
    "moderate(\"Hope you're having a good day you cute piece of shit\")  # 0\n",
    "moderate(\"hello there\")\n",
    "moderate(\"today was fine\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "44607cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] END .......................C=0.1, class_weight=balanced; total time=   0.1s\n",
      "[CV] END .......................C=0.1, class_weight=balanced; total time=   0.1s\n",
      "[CV] END ...........................C=0.1, class_weight=None; total time=   0.1s\n",
      "[CV] END .......................C=0.1, class_weight=balanced; total time=   0.1s\n",
      "[CV] END ...........................C=0.1, class_weight=None; total time=   0.1s\n",
      "[CV] END ...........................C=0.1, class_weight=None; total time=   0.1s\n",
      "[CV] END .............................C=1, class_weight=None; total time=   0.3s\n",
      "[CV] END .........................C=1, class_weight=balanced; total time=   0.2s\n",
      "[CV] END .............................C=1, class_weight=None; total time=   0.4s\n",
      "[CV] END .............................C=1, class_weight=None; total time=   0.3s\n",
      "[CV] END .........................C=1, class_weight=balanced; total time=   0.3s\n",
      "[CV] END .........................C=1, class_weight=balanced; total time=   0.3s\n",
      "[CV] END ............................C=10, class_weight=None; total time=   1.1s\n",
      "[CV] END ............................C=10, class_weight=None; total time=   1.1s\n",
      "[CV] END ........................C=10, class_weight=balanced; total time=   1.1s\n",
      "[CV] END ........................C=10, class_weight=balanced; total time=   1.1s\n",
      "[CV] END ............................C=10, class_weight=None; total time=   2.2s\n",
      "[CV] END ........................C=10, class_weight=balanced; total time=   2.0s\n",
      "Best Params: {'C': 0.1, 'class_weight': None}\n"
     ]
    }
   ],
   "source": [
    "# hyperparamter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [ 0.1, 1, 10],  # Regularization strength\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    estimator=LinearSVC(max_iter=5000),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # 5-fold cross-validation\n",
    "    scoring='f1',  # You can also use 'accuracy', 'precision', etc.\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "grid.fit(X, y)\n",
    "# grid = GridSearchCV(SVC(), param_grid, cv=3)\n",
    "#grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f4221a",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "cc8ac595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cleaned_tokens",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "tokenized_cleaned_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cleaned_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "214f19cb-8065-4b6c-96ea-042ab591ebde",
       "rows": [
        [
         "0",
         "see this sums up your personal wiki and administrator of wiki class piece of shit even lower than that you as an administrator just violated civil and you remind of thousand wikipedia civil obedience blah blah blah well i am weeelllll are aware of all your policies and since none of you administrator cared to see that the article what a slut a complete fake and full of lies and instead of that accused me i also forgot to acknowledge even a bit of your policies why should i care about the policies when admins violate it themselves btw if even one of you had cared to look the article and its fake references and no references even the issue would have solved where are you ago this again sums up your class most of you admins or who even veteran admins are either illiterate or did not have a job to do retired and classes a bunch of arrogant idiots some be responsible and educated i know a few but most of you clowns don't even listen to them a clear proof me i have been watching tennis and following records for decades so once i saw that article i refused it going directly for a fan puffer proven also by the references it used like the google tennis warehouse fan chat room website some blogs etc btw i think i also am ahead in terms of academic degree than most half of you i wrote this long comment because i know instead of punishing myself the article creator and editing all its ridiculously funny claims about federer you would read this and come back and teach me blah blah wiki civil wiki etc you just wasted your time",
         "1",
         "['see', 'sum', 'personal', 'wiki', 'administrator', 'wiki', 'class', 'piece', 'shit', 'even', 'lower', 'administrator', 'violated', 'civil', 'remind', 'thousand', 'wikipedia', 'civil', 'obedience', 'blah', 'blah', 'blah', 'well', 'weeelllll', 'aware', 'policy', 'since', 'none', 'administrator', 'cared', 'see', 'article', 'slut', 'complete', 'fake', 'full', 'lie', 'instead', 'accused', 'also', 'forgot', 'acknowledge', 'even', 'bit', 'policy', 'care', 'policy', 'admins', 'violate', 'btw', 'even', 'one', 'cared', 'look', 'article', 'fake', 'reference', 'reference', 'even', 'issue', 'would', 'solved', 'ago', 'sum', 'class', 'admins', 'even', 'veteran', 'admins', 'either', 'illiterate', 'job', 'retired', 'class', 'bunch', 'arrogant', 'idiot', 'responsible', 'educated', 'know', 'clown', 'even', 'listen', 'clear', 'proof', 'watching', 'tennis', 'following', 'record', 'decade', 'saw', 'article', 'refused', 'going', 'directly', 'fan', 'puffer', 'proven', 'also', 'reference', 'used', 'like', 'google', 'tennis', 'warehouse', 'fan', 'chat', 'room', 'website', 'blog', 'etc', 'btw', 'think', 'also', 'ahead', 'term', 'academic', 'degree', 'half', 'wrote', 'long', 'comment', 'know', 'instead', 'punishing', 'article', 'creator', 'editing', 'ridiculously', 'funny', 'claim', 'federer', 'would', 'read', 'come', 'back', 'teach', 'blah', 'blah', 'wiki', 'civil', 'wiki', 'etc', 'wasted', 'time']",
         "see sum personal wiki administrator wiki class piece shit even lower administrator violated civil remind thousand wikipedia civil obedience blah blah blah well weeelllll aware policy since none administrator cared see article slut complete fake full lie instead accused also forgot acknowledge even bit policy care policy admins violate btw even one cared look article fake reference reference even issue would solved ago sum class admins even veteran admins either illiterate job retired class bunch arrogant idiot responsible educated know clown even listen clear proof watching tennis following record decade saw article refused going directly fan puffer proven also reference used like google tennis warehouse fan chat room website blog etc btw think also ahead term academic degree half wrote long comment know instead punishing article creator editing ridiculously funny claim federer would read come back teach blah blah wiki civil wiki etc wasted time",
         "see this sums up your personal wiki and administrator of wiki class piece of shit even lower than that you as an administrator just violated civil and you remind of thousand wikipedia civil obedience blah blah blah well i am weeelllll are aware of all your policies and since none of you administrator cared to see that the article what a slut a complete fake and full of lies and instead of that accused me i also forgot to acknowledge even a bit of your policies why should i care about the policies when admins violate it themselves btw if even one of you had cared to look the article and its fake references and no references even the issue would have solved where are you ago this again sums up your class most of you admins or who even veteran admins are either illiterate or did not have a job to do retired and classes a bunch of arrogant idiots some be responsible and educated i know a few but most of you clowns don't even listen to them a clear proof me i have been watching tennis and following records for decades so once i saw that article i refused it going directly for a fan puffer proven also by the references it used like the google tennis warehouse fan chat room website some blogs etc btw i think i also am ahead in terms of academic degree than most half of you i wrote this long comment because i know instead of punishing myself the article creator and editing all its ridiculously funny claims about federer you would read this and come back and teach me blah blah wiki civil wiki etc you just wasted your time"
        ],
        [
         "1",
         "the beach is calling weekend has already begun celebrate happyjohnmccainday trumpisatraitor votebluetosaveamerica",
         "1",
         "['beach', 'calling', 'weekend', 'already', 'begun', 'celebrate', 'happyjohnmccainday', 'trumpisatraitor', 'votebluetosaveamerica']",
         "beach calling weekend already begun celebrate happyjohnmccainday trumpisatraitor votebluetosaveamerica",
         "the beach is calling weekend has already begun celebrate happyjohnmccainday trumpisatraitor votebluetosaveamerica"
        ],
        [
         "2",
         "well you lord fuck this faggot",
         "1",
         "['well', 'lord', 'fuck', 'faggot']",
         "well lord fuck faggot",
         "well you lord fuck this faggot"
        ],
        [
         "3",
         "grave treachery is also the next most significant quality of southern goat ropers",
         "1",
         "['grave', 'treachery', 'also', 'next', 'significant', 'quality', 'southern', 'goat', 'roper']",
         "grave treachery also next significant quality southern goat roper",
         "grave treachery is also the next most significant quality of southern goat ropers"
        ],
        [
         "4",
         "bytch you voice stupid as f ck first off if those were my got damn kids note i t receive any kids but if i did d be calling my girlfriend up and letting her know that my kids would be referring to her as mammy shannon from now on and if the bitch didn t like it too mother skin high risk if her talentless ass didn t have to pay for d ck she wouldn t be having all these f skin issues in the first place but apparently her same sex game must be awfully wack because ve noticed that she never keeps any man s e'er been with interested in her for more than a few years then she either gets dumped or cheated on and truth be tell shannon the lima is times finer than j lo is and this whore thinks she can stand up and walk on stage and act as if her pussy just gleaming like the damn day she what a slut born are not no man going to touch that stinky fish ass sandwich no way",
         "1",
         "['bytch', 'voice', 'stupid', 'f', 'ck', 'first', 'got', 'damn', 'kid', 'note', 'receive', 'kid', 'calling', 'girlfriend', 'letting', 'know', 'kid', 'would', 'referring', 'mammy', 'shannon', 'bitch', 'like', 'mother', 'skin', 'high', 'risk', 'talentless', 'as', 'pay', 'ck', 'f', 'skin', 'issue', 'first', 'place', 'apparently', 'sex', 'game', 'must', 'awfully', 'wack', 'noticed', 'never', 'keep', 'man', 'interested', 'year', 'either', 'get', 'dumped', 'cheated', 'truth', 'tell', 'shannon', 'lima', 'time', 'finer', 'j', 'lo', 'whore', 'think', 'stand', 'walk', 'stage', 'act', 'pussy', 'gleaming', 'like', 'damn', 'day', 'slut', 'born', 'man', 'going', 'touch', 'stinky', 'fish', 'as', 'sandwich', 'way']",
         "bytch voice stupid f ck first got damn kid note receive kid calling girlfriend letting know kid would referring mammy shannon bitch like mother skin high risk talentless as pay ck f skin issue first place apparently sex game must awfully wack noticed never keep man interested year either get dumped cheated truth tell shannon lima time finer j lo whore think stand walk stage act pussy gleaming like damn day slut born man going touch stinky fish as sandwich way",
         "bytch you voice stupid as f ck first off if those were my got damn kids note i t receive any kids but if i did d be calling my girlfriend up and letting her know that my kids would be referring to her as mammy shannon from now on and if the bitch didn t like it too mother skin high risk if her talentless ass didn t have to pay for d ck she wouldn t be having all these f skin issues in the first place but apparently her same sex game must be awfully wack because ve noticed that she never keeps any man s e'er been with interested in her for more than a few years then she either gets dumped or cheated on and truth be tell shannon the lima is times finer than j lo is and this whore thinks she can stand up and walk on stage and act as if her pussy just gleaming like the damn day she what a slut born are not no man going to touch that stinky fish ass sandwich no way"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "      <th>tokenized_cleaned_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>see this sums up your personal wiki and admini...</td>\n",
       "      <td>1</td>\n",
       "      <td>[see, sum, personal, wiki, administrator, wiki...</td>\n",
       "      <td>see sum personal wiki administrator wiki class...</td>\n",
       "      <td>see this sums up your personal wiki and admini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the beach is calling weekend has already begun...</td>\n",
       "      <td>1</td>\n",
       "      <td>[beach, calling, weekend, already, begun, cele...</td>\n",
       "      <td>beach calling weekend already begun celebrate ...</td>\n",
       "      <td>the beach is calling weekend has already begun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well you lord fuck this faggot</td>\n",
       "      <td>1</td>\n",
       "      <td>[well, lord, fuck, faggot]</td>\n",
       "      <td>well lord fuck faggot</td>\n",
       "      <td>well you lord fuck this faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grave treachery is also the next most signific...</td>\n",
       "      <td>1</td>\n",
       "      <td>[grave, treachery, also, next, significant, qu...</td>\n",
       "      <td>grave treachery also next significant quality ...</td>\n",
       "      <td>grave treachery is also the next most signific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bytch you voice stupid as f ck first off if th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[bytch, voice, stupid, f, ck, first, got, damn...</td>\n",
       "      <td>bytch voice stupid f ck first got damn kid not...</td>\n",
       "      <td>bytch you voice stupid as f ck first off if th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Label  \\\n",
       "0  see this sums up your personal wiki and admini...      1   \n",
       "1  the beach is calling weekend has already begun...      1   \n",
       "2                     well you lord fuck this faggot      1   \n",
       "3  grave treachery is also the next most signific...      1   \n",
       "4  bytch you voice stupid as f ck first off if th...      1   \n",
       "\n",
       "                                      cleaned_tokens  \\\n",
       "0  [see, sum, personal, wiki, administrator, wiki...   \n",
       "1  [beach, calling, weekend, already, begun, cele...   \n",
       "2                         [well, lord, fuck, faggot]   \n",
       "3  [grave, treachery, also, next, significant, qu...   \n",
       "4  [bytch, voice, stupid, f, ck, first, got, damn...   \n",
       "\n",
       "                              tokenized_cleaned_text  \\\n",
       "0  see sum personal wiki administrator wiki class...   \n",
       "1  beach calling weekend already begun celebrate ...   \n",
       "2                              well lord fuck faggot   \n",
       "3  grave treachery also next significant quality ...   \n",
       "4  bytch voice stupid f ck first got damn kid not...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  see this sums up your personal wiki and admini...  \n",
       "1  the beach is calling weekend has already begun...  \n",
       "2                     well you lord fuck this faggot  \n",
       "3  grave treachery is also the next most signific...  \n",
       "4  bytch you voice stupid as f ck first off if th...  "
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning data for models that require sequences of integers as text input  (LSTM/Deep Nets)\n",
    "import re\n",
    "\n",
    "def clean_for_lstm(text):\n",
    "    text = re.sub(r'(rt)?\\s?@\\w+:?', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\'\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "# clean_tweet = tweet_column.str.replace(r'(rt)?\\s?@\\w+:?', ' ', regex=True).str.replace(r'http.+', ' ', regex=True).str.replace(r'\\W+', ' ', regex=True)\n",
    "training_data_df['cleaned_text'] = training_data_df['Content'].apply(clean_for_lstm)\n",
    "training_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e5bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_64\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_64\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_64                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_188 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_189 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_190 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_64 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_64                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_64 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_188 (\u001b[38;5;33mDense\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_189 (\u001b[38;5;33mDense\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_190 (\u001b[38;5;33mDense\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m356/356\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.6753 - loss: 0.6277 - val_accuracy: 0.8697 - val_loss: 0.4811\n",
      "Epoch 2/5\n",
      "\u001b[1m356/356\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - accuracy: 0.8977 - loss: 0.4351 - val_accuracy: 0.8752 - val_loss: 0.4089\n",
      "Epoch 3/5\n",
      "\u001b[1m356/356\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.9268 - loss: 0.3450 - val_accuracy: 0.8891 - val_loss: 0.3626\n",
      "Epoch 4/5\n",
      "\u001b[1m356/356\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 39ms/step - accuracy: 0.9395 - loss: 0.2857 - val_accuracy: 0.8756 - val_loss: 0.3731\n",
      "Epoch 5/5\n",
      "\u001b[1m356/356\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.9462 - loss: 0.2507 - val_accuracy: 0.8879 - val_loss: 0.3365\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.84      0.88      3126\n",
      "           1       0.86      0.95      0.90      3186\n",
      "\n",
      "    accuracy                           0.89      6312\n",
      "   macro avg       0.90      0.89      0.89      6312\n",
      "weighted avg       0.90      0.89      0.89      6312\n",
      "\n",
      "[[2616  510]\n",
      " [ 170 3016]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LSTM requires sequences of integers, NOT vectors\n",
    "texts = training_data_df['cleaned_text'].values\n",
    "labels = training_data_df['Label'].values\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, padding='post', maxlen=50)  # 50 = max length\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64, input_length=50),\n",
    "    Bidirectional(LSTM(64, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    #Dense(16, activation='softmax'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(4, activation='softmax'),\n",
    "\n",
    "   \n",
    "\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=5, # 7 epochs: 77% , 5 epochs had 77% accuracy, 3 epochs has 79%\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "legend = {0: \"' is permitted\", 1: \"' has been flagged for hate speech.\"}\n",
    "\n",
    "def moderate_lstm(tweet):\n",
    "    cleaned = clean_for_lstm(tweet)\n",
    "    print(cleaned)\n",
    "    seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    padded = pad_sequences(seq, padding='post', maxlen=50)\n",
    "    pred = model.predict(padded)[0][0]\n",
    "    xx = lambda x : 1 if x > 0.7 else 0\n",
    "    print('The tweet: \"',tweet,legend[xx(pred)], \". Value: \",pred )\n",
    "    return 1 if pred > 0.7 else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "83f970cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      3186\n",
      "           1       0.88      0.90      0.89      3186\n",
      "\n",
      "    accuracy                           0.89      6372\n",
      "   macro avg       0.89      0.89      0.89      6372\n",
      "weighted avg       0.89      0.89      0.89      6372\n",
      "\n",
      "[[2802  384]\n",
      " [ 306 2880]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4361199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello there\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "The tweet: \" hello there ' is permitted . Value:  0.17511961\n",
      "today was fine\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "The tweet: \" today was fine ' is permitted . Value:  0.1257655\n",
      "twirl\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "The tweet: \" twirl ' has been flagged for hate speech. . Value:  0.78289557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model \n",
    "moderate_lstm(\"hello there\")\n",
    "moderate_lstm(\"today was fine\")\n",
    "moderate_lstm(\"twirl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
