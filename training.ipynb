{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec761811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "\n",
    "\n",
    "# save for later\n",
    "#from sklearn.decomposition import PCA\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.datasets import make_classification\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import classification_report\n",
    "\n",
    "#os.chdir(r\"C:\\Users\\raned\\Documents\\GitHub\\PostModeration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29653db5",
   "metadata": {},
   "source": [
    "This notebook will start off with preprocessing the two csv files to train different supervised learning models. \n",
    "- Removal of usernames, URLs, and special characters\n",
    "- Lowercasing text\n",
    "- Tokenization (nltk or spaCy): breaking text into smaller units \n",
    "- Stopword removal: remove common words that become index terms (\"and\", \"or\", \"the\", \"in\")\n",
    "- Lemmatization: reduces words to their base or dictionary form\n",
    "- TF-IDF vectorization for feature extraction: a technique that converts text data into numerical vectors, representing the importance of words in a document relative to a collection of documents, by combining term frequency with inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a025d07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== df1: Dataset1 ===\n",
      "         Unnamed: 0         count   hate_speech  offensive_language  \\\n",
      "count  24783.000000  24783.000000  24783.000000        24783.000000   \n",
      "mean   12681.192027      3.243473      0.280515            2.413711   \n",
      "std     7299.553863      0.883060      0.631851            1.399459   \n",
      "min        0.000000      3.000000      0.000000            0.000000   \n",
      "25%     6372.500000      3.000000      0.000000            2.000000   \n",
      "50%    12703.000000      3.000000      0.000000            3.000000   \n",
      "75%    18995.500000      3.000000      0.000000            3.000000   \n",
      "max    25296.000000      9.000000      7.000000            9.000000   \n",
      "\n",
      "            neither         class  \n",
      "count  24783.000000  24783.000000  \n",
      "mean       0.549247      1.110277  \n",
      "std        1.113299      0.462089  \n",
      "min        0.000000      0.000000  \n",
      "25%        0.000000      1.000000  \n",
      "50%        0.000000      1.000000  \n",
      "75%        0.000000      1.000000  \n",
      "max        9.000000      2.000000  \n",
      "(24783, 7)\n",
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24783 entries, 0 to 24782\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Unnamed: 0          24783 non-null  int64 \n",
      " 1   count               24783 non-null  int64 \n",
      " 2   hate_speech         24783 non-null  int64 \n",
      " 3   offensive_language  24783 non-null  int64 \n",
      " 4   neither             24783 non-null  int64 \n",
      " 5   class               24783 non-null  int64 \n",
      " 6   tweet               24783 non-null  object\n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "Index(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither',\n",
      "       'class', 'tweet'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"TrainingData/labeled_data.csv\")\n",
    "\n",
    "print(\"=== df1: Dataset1 ===\")\n",
    "print(df1.describe())\n",
    "print(df1.shape)\n",
    "print(df1.head())\n",
    "print(df1.info())\n",
    "print(df1.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112d42c",
   "metadata": {},
   "source": [
    "**count**: number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were\n",
    "\n",
    "**hate_speech**: number of CF users who judged the tweet to be hate speech\n",
    "\n",
    "**offensive_language**: number of CF users who judged the tweet to be offensive\n",
    "\n",
    "**neither**: number of CF users who judged the tweet to be neither offensive nor non-offensive\n",
    "\n",
    "**class**: class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7a023f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    !!! rt @mayasolovely: as a woman you shouldn't...\n",
       "1    !!!!! rt @mleew17: boy dats cold...tyga dwn ba...\n",
       "2    !!!!!!! rt @urkindofbrand dawg!!!! rt @80sbaby...\n",
       "3    !!!!!!!!! rt @c_g_anderson: @viva_based she lo...\n",
       "4    !!!!!!!!!!!!! rt @shenikaroberts: the shit you...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrubbing text: removing usernames, URLs, special characters and ensuring all text is lowercase\n",
    "tweet_column = df1['tweet'].astype(str).str.casefold()  # lowercase\n",
    "tweet_column.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b94924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ,rt,as,a,woman,you,shouldn,t,complain,about,cl...\n",
       "1    ,rt,boy,dats,cold,tyga,dwn,bad,for,cuffin,dat,...\n",
       "2    ,rt,dawg,rt,you,ever,fuck,a,bitch,and,she,star...\n",
       "3                           ,rt,she,look,like,a,tranny\n",
       "4    ,rt,the,shit,you,hear,about,me,might,be,true,o...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removes usernames first, urls, then any special characters\n",
    "clean_tweet = tweet_column.str.replace(r'@\\w+:?', ' ', regex=True).str.replace(r'http.+', ',', regex=True).str.replace(r'\\W+', ',', regex=True)\n",
    "clean_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b63046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\raned\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\raned\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\raned\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\raned\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [woman, complain, cleaning, house, amp, man, a...\n",
      "1    [boy, dat, cold, tyga, dwn, bad, cuffin, dat, ...\n",
      "2    [dawg, ever, fuck, bitch, start, cry, confused...\n",
      "3                                 [look, like, tranny]\n",
      "4    [shit, hear, might, true, might, faker, bitch,...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#tokenization, stop words, and lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) #stopwords\n",
    "\n",
    "def clean_tokenize(text): \n",
    "    tokens = word_tokenize(text)  # Keeps contractions like \"don't\"; tokenization\n",
    "    tokens = [t.lower() for t in tokens if t.isalpha() or \"'\" in t]  # keep letters + contractions\n",
    "    tokens = [t for t in tokens if t != \"rt\" and t not in stop_words]  # remove 'rt' and stopwords\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens] #lemmatization\n",
    "    return lemmatized\n",
    "\n",
    "cleaned_tokens = clean_tweet.apply(clean_tokenize)\n",
    "print(cleaned_tokens.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eab4ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaaaaaaaand' 'aaahhhhh' 'aahahah' 'aaliyah' 'aan' 'aap' 'aaron'\n",
      " 'aaronmacgruder' 'ab' 'abandonado' 'abbey' 'abby' 'abc' 'abdelka'\n",
      " 'abduction' 'abdullah' 'abdurahman' 'abed' 'abel' 'aberdeen' 'ability'\n",
      " 'able' 'abo' 'aborted' 'abortion' 'abouta' 'abouttime' 'abraham' 'absent'\n",
      " 'absolute' 'absolutely' 'absoluteyvile' 'absolved' 'abstract' 'absurd'\n",
      " 'abt' 'abu' 'abundance' 'abuse' 'abused' 'abuser' 'abusive' 'ac' 'aca'\n",
      " 'acab' 'academic' 'accelerated' 'accent' 'accept' 'acceptable'\n",
      " 'acceptance' 'accepted' 'access' 'accessible' 'accessorize' 'accessory'\n",
      " 'accident' 'accidentally' 'accipiter' 'accipitridae' 'accnt' 'accolade'\n",
      " 'accompanied' 'accord' 'according' 'accordingly' 'account' 'accountable'\n",
      " 'accountant' 'acct' 'accuracy' 'accurate' 'accurately' 'accused'\n",
      " 'accuses' 'accustomed' 'acdc' 'ace' 'aceptar' 'aceves' 'ach' 'achieve'\n",
      " 'achilles' 'aching' 'acid' 'ackin' 'acknowledge' 'acknowledged'\n",
      " 'acknowledging' 'acl' 'acne' 'acoustic' 'acquire' 'acre' 'acronym'\n",
      " 'across' 'act' 'acted' 'actin']\n",
      "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0               0      3            0                   0        3      2   \n",
      "1               1      3            0                   3        0      1   \n",
      "2               2      3            0                   3        0      1   \n",
      "3               3      3            0                   2        1      1   \n",
      "4               4      6            0                   6        0      1   \n",
      "...           ...    ...          ...                 ...      ...    ...   \n",
      "24778       25291      3            0                   2        1      1   \n",
      "24779       25292      3            0                   1        2      2   \n",
      "24780       25294      3            0                   3        0      1   \n",
      "24781       25295      6            0                   6        0      1   \n",
      "24782       25296      3            0                   0        3      2   \n",
      "\n",
      "                                                   tweet  \\\n",
      "0      !!! RT @mayasolovely: As a woman you shouldn't...   \n",
      "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
      "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
      "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
      "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
      "...                                                  ...   \n",
      "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...   \n",
      "24779  you've gone and broke the wrong heart baby, an...   \n",
      "24780  young buck wanna eat!!.. dat nigguh like I ain...   \n",
      "24781              youu got wild bitches tellin you lies   \n",
      "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...   \n",
      "\n",
      "                                            cleaned_text  \n",
      "0      woman complain cleaning house amp man always t...  \n",
      "1         boy dat cold tyga dwn bad cuffin dat hoe place  \n",
      "2           dawg ever fuck bitch start cry confused shit  \n",
      "3                                       look like tranny  \n",
      "4            shit hear might true might faker bitch told  \n",
      "...                                                  ...  \n",
      "24778               muthaf right tl mine bible scripture  \n",
      "24779    gone broke wrong heart baby drove redneck crazy  \n",
      "24780  young buck wan na eat dat nigguh like aint fuc...  \n",
      "24781                     youu got wild bitch tellin lie  \n",
      "24782  ruffled ntac eileen dahlia beautiful color com...  \n",
      "\n",
      "[24783 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df1['cleaned_text'] = cleaned_tokens.apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df1['cleaned_text'])\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(feature_names[:100])  \n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d630e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (27000, 32988)\n",
      "['aa' 'aaa' 'aaaa' 'aaaaa' 'aaaaaaaaaaaaaaaaa' 'aaaaaaacopyrighta'\n",
      " 'aaaaaaareaareaaaaaaaaaaaaaa' 'aaaaarrrrrggggghhhhh' 'aaaacg'\n",
      " 'aaadonaaat' 'aaah' 'aaand' 'aachen' 'aaeyou' 'aag' 'aah' 'aaib' 'aaj'\n",
      " 'aak' 'aalukkoru' 'aand' 'aanti' 'aap' 'aardvark' 'aaron' 'aaroncrick'\n",
      " 'aarp' 'aau' 'ab' 'aba' 'aback' 'abacus' 'abandon' 'abandoned' 'abash'\n",
      " 'abated' 'abaxial' 'abb' 'abba' 'abbey' 'abbott' 'abbreviated'\n",
      " 'abbreviation' 'abc' 'abd' 'abdf' 'abdomen' 'abduce' 'abdul' 'abdullah'\n",
      " 'abe' 'abecedary' 'abeh' 'abel' 'abelson' 'aberdeen' 'abet' 'abeyance'\n",
      " 'abf' 'abhishek' 'abhorrent' 'abidance' 'abide' 'abiding' 'abigail'\n",
      " 'ability' 'abiogenic' 'abject' 'abk' 'abkhazia' 'able' 'abm' 'abnegation'\n",
      " 'abner' 'abnormal' 'abnormality' 'aboard' 'abode' 'abolish' 'abolished'\n",
      " 'abolishment' 'abolitionist' 'abominable' 'abominably' 'abominate'\n",
      " 'abomination' 'aboridzinima' 'aboriginal' 'aborigine' 'abort' 'aborted'\n",
      " 'abortion' 'abortive' 'abortively' 'abound' 'abp' 'abraham' 'abrahamic'\n",
      " 'abramowicz' 'abrams' 'abrandnewday' 'abrar' 'abrasive' 'abrasiveness'\n",
      " 'abroad' 'abrogated' 'abrogation' 'abrupt' 'abruptly' 'absence' 'absent'\n",
      " 'absolkutely' 'absolute' 'absolutely' 'absolutest' 'absorb' 'absorbed'\n",
      " 'absorption' 'abstain' 'abstained' 'abstaining' 'abstinence' 'abstract'\n",
      " 'abstraction' 'absurd' 'absurdity' 'absurdly' 'abu' 'abubakar' 'abuela'\n",
      " 'abundance' 'abundant' 'abundantly' 'abuse' 'abused' 'abuser' 'abusing'\n",
      " 'abusive' 'abysmal' 'abyss' 'ac' 'acacia' 'academe' 'academia' 'academic'\n",
      " 'academically' 'academy' 'acas' 'acat' 'acc' 'acceded' 'accelerated'\n",
      " 'acceleration' 'accelerator' 'accent' 'accented' 'accept' 'acceptable'\n",
      " 'acceptance' 'accepted' 'acceptibility' 'accepting' 'accepts' 'access'\n",
      " 'accessed' 'accessibility' 'accessible' 'accession' 'accessory'\n",
      " 'accident' 'accidental' 'accidentally' 'acclaim' 'acclaimed'\n",
      " 'acclamation' 'accommodate' 'accommodating' 'accommodative' 'accompanied'\n",
      " 'accompany' 'accompanying' 'accomplice' 'accomplish' 'accomplished'\n",
      " 'accomplishes' 'accomplishing' 'accomplishment' 'accord' 'accordance'\n",
      " 'accorded' 'according' 'accordingly' 'accordion' 'accost' 'account'\n",
      " 'accountability' 'accountable' 'accountant' 'accra' 'accrafloods'\n",
      " 'accreditation' 'accredited' 'accretion' 'accrue' 'acct' 'accumulate'\n",
      " 'accumulated' 'accumulation' 'accuracy' 'accurate' 'accurately'\n",
      " 'accusation' 'accusatory' 'accuse' 'accused' 'accuser' 'accuses'\n",
      " 'accusing' 'ace' 'acerbic' 'acetaminophen' 'acetate' 'achaemenid' 'ache'\n",
      " 'achebe' 'ached' 'acheson' 'achieve' 'achieved' 'achievement' 'achieving'\n",
      " 'achmad' 'aci' 'acici' 'acid' 'acidbase' 'acipa' 'acje' 'acknowledge'\n",
      " 'acknowledged' 'acknowledgement' 'acknowledges' 'acknowledging'\n",
      " 'acknowledgment' 'ackoz' 'aclarification' 'aclemfaal' 'aclu' 'acm' 'acme'\n",
      " 'acne' 'acolyte' 'acopyrighta' 'acopyrightphotocred'\n",
      " 'acopyrightreversegraffiti' 'acorruptionfreeindia' 'acosta' 'acoustic'\n",
      " 'acquaint' 'acquaintance' 'acquire' 'acquired' 'acquiring' 'acquisition'\n",
      " 'acquitted' 'acre' 'acrid' 'acrimonious' 'acronym' 'across' 'acrusadera'\n",
      " 'act' 'acted' 'acting' 'action' 'actionable' 'actionbioscience'\n",
      " 'actionoldid' 'activate' 'activated' 'activating' 'activation' 'active'\n",
      " 'activecollab' 'actively' 'activism' 'activist' 'activity' 'actor'\n",
      " 'actress' 'actual' 'actually' 'actuallynofilter' 'actuate' 'actuator'\n",
      " 'acuminata' 'acupuncture' 'acute' 'acutely' 'acw' 'acyclovir' 'ad' 'ada'\n",
      " 'adage' 'adam' 'adamant' 'adapt' 'adaptation' 'adapted' 'adapter'\n",
      " 'adapting' 'adaptive' 'adapts' 'adar' 'adav' 'add' 'addams' 'added'\n",
      " 'addendum' 'adderall' 'addict' 'addiction' 'addictive' 'addie' 'adding'\n",
      " 'addition' 'additional' 'additionally' 'addled' 'address' 'addressed'\n",
      " 'addressing' 'adduce' 'adelaide' 'adele' 'aden' 'adenauer' 'adept'\n",
      " 'adequate' 'adequately' 'adf' 'adhd' 'adhere' 'adhered' 'adherence'\n",
      " 'adherent' 'adheres' 'adhering' 'adi' 'adiant' 'adios' 'adithya' 'adj'\n",
      " 'adjacent' 'adjectival' 'adjective' 'adjoined' 'adjudicate' 'adjudicated'\n",
      " 'adjust' 'adjustable' 'adjusted' 'adjusting' 'adjustment' 'adjutant'\n",
      " 'adl' 'adler' 'adm' 'admin' 'administered' 'administering'\n",
      " 'administration' 'administrative' 'administrator' 'admins' 'admirable'\n",
      " 'admiral' 'admiration' 'admire' 'admired' 'admirer' 'admires' 'admiring'\n",
      " 'admission' 'admisttraot' 'admit' 'admits' 'admittance' 'admitted'\n",
      " 'admittedly' 'admitting' 'admixture' 'admonish' 'admonishing'\n",
      " 'admonition' 'admonitory' 'adnan' 'adobe' 'adolescent' 'adolf'\n",
      " 'adolfadolf' 'adopt' 'adopted' 'adopter' 'adopting' 'adoption' 'adoptive'\n",
      " 'adopts' 'adorable' 'adoration' 'adore' 'adorn' 'adorned' 'adorno' 'adp'\n",
      " 'adrian' 'adrien' 'adrienne' 'adrift' 'adroit' 'adsense' 'adsl'\n",
      " 'adsorbing' 'adult' 'adultdaycarefortrump' 'adulterate' 'adulteration'\n",
      " 'adulteress' 'adulterous' 'adultery' 'adulthood' 'advance' 'advanced'\n",
      " 'advancement' 'advancing' 'advantage' 'advantageously' 'advent'\n",
      " 'adventure' 'adventured' 'adventurer' 'adversary' 'adverse' 'adversely'\n",
      " 'adversity' 'advert' 'advertise' 'advertised' 'advertisement'\n",
      " 'advertiser' 'advertises' 'advertising' 'advice'\n",
      " 'adviceforyoungfeminists' 'advisably' 'advise' 'advised' 'advisement'\n",
      " 'adviser' 'advising' 'advisor' 'advisory' 'advocacy' 'advocate'\n",
      " 'advocated' 'advocating' 'adware' 'adz' 'aee' 'aegis' 'aeneas' 'aerie'\n",
      " 'aero' 'aerobic' 'aerobics' 'aerodrome' 'aeronautical' 'aeropagitica'\n",
      " 'aesculapian' 'aesthetic' 'aesthetically' 'af' 'afaik' 'afar' 'afc' 'afd'\n",
      " 'affair' 'affect' 'affected' 'affecting' 'affection' 'affidavit'\n",
      " 'affiliate' 'affiliated' 'affiliation' 'affinity' 'affirm' 'affirmation'\n",
      " 'affirmative' 'affirmed']\n"
     ]
    }
   ],
   "source": [
    "#PREPROCESSING FOR HateSpeechDatasetBalanced.csv\n",
    "\n",
    "#Load dataset and take a 27,000-row sample; easier to have random 27,000 samples due to how big the actual dataset is \n",
    "df = pd.read_csv(\"TrainingData/HateSpeechDatasetBalanced.csv\")\n",
    "df_subset = df.sample(n=27000, random_state=42).copy()\n",
    "\n",
    "df_subset['Content'] = df_subset['Content'].astype(str).str.casefold()\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    tokens = word_tokenize(text)  # splits into words and keeps contractions\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # keep only alphabetic tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "df_subset['cleaned_tokens'] = df_subset['Content'].apply(clean_tokenize)\n",
    "df_subset['cleaned_text'] = df_subset['cleaned_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "df_subset['cleaned_text'] = df_subset['cleaned_text'].fillna('')\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df_subset['cleaned_text'])  # Features\n",
    "y = df_subset['Label']  # Target labels\n",
    "\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "print(tfidf.get_feature_names_out()[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
