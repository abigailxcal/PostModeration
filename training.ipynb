{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec761811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "\n",
    "# save for later\n",
    "#from sklearn.decomposition import PCA\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.datasets import make_classification\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import classification_report\n",
    "\n",
    "os.chdir(r\"C:\\Users\\raned\\Documents\\GitHub\\PostModeration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29653db5",
   "metadata": {},
   "source": [
    "This notebook will start off with preprocessing the two csv files to train different supervised learning models. \n",
    "- Removal of usernames, URLs, and special characters\n",
    "- Lowercasing text\n",
    "- Tokenization (nltk or spaCy): breaking text into smaller units \n",
    "- Stopword removal: remove common words that become index terms (\"and\", \"or\", \"the\", \"in\")\n",
    "- Lemmatization: reduces words to their base or dictionary form\n",
    "- TF-IDF vectorization for feature extraction: a technique that converts text data into numerical vectors, representing the importance of words in a document relative to a collection of documents, by combining term frequency with inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a025d07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0         count   hate_speech  offensive_language  \\\n",
      "count  24783.000000  24783.000000  24783.000000        24783.000000   \n",
      "mean   12681.192027      3.243473      0.280515            2.413711   \n",
      "std     7299.553863      0.883060      0.631851            1.399459   \n",
      "min        0.000000      3.000000      0.000000            0.000000   \n",
      "25%     6372.500000      3.000000      0.000000            2.000000   \n",
      "50%    12703.000000      3.000000      0.000000            3.000000   \n",
      "75%    18995.500000      3.000000      0.000000            3.000000   \n",
      "max    25296.000000      9.000000      7.000000            9.000000   \n",
      "\n",
      "            neither         class  \n",
      "count  24783.000000  24783.000000  \n",
      "mean       0.549247      1.110277  \n",
      "std        1.113299      0.462089  \n",
      "min        0.000000      0.000000  \n",
      "25%        0.000000      1.000000  \n",
      "50%        0.000000      1.000000  \n",
      "75%        0.000000      1.000000  \n",
      "max        9.000000      2.000000  \n",
      "(24783, 7)\n",
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24783 entries, 0 to 24782\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Unnamed: 0          24783 non-null  int64 \n",
      " 1   count               24783 non-null  int64 \n",
      " 2   hate_speech         24783 non-null  int64 \n",
      " 3   offensive_language  24783 non-null  int64 \n",
      " 4   neither             24783 non-null  int64 \n",
      " 5   class               24783 non-null  int64 \n",
      " 6   tweet               24783 non-null  object\n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"TrainingData/labeled_data.csv\")\n",
    "print(df.describe())\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112d42c",
   "metadata": {},
   "source": [
    "**count**: number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were\n",
    "\n",
    "**hate_speech**: number of CF users who judged the tweet to be hate speech\n",
    "\n",
    "**offensive_language**: number of CF users who judged the tweet to be offensive\n",
    "\n",
    "**neither**: number of CF users who judged the tweet to be neither offensive nor non-offensive\n",
    "\n",
    "**class**: class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d7a023f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    !!! rt @mayasolovely: as a woman you shouldn't...\n",
       "1    !!!!! rt @mleew17: boy dats cold...tyga dwn ba...\n",
       "2    !!!!!!! rt @urkindofbrand dawg!!!! rt @80sbaby...\n",
       "3    !!!!!!!!! rt @c_g_anderson: @viva_based she lo...\n",
       "4    !!!!!!!!!!!!! rt @shenikaroberts: the shit you...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrubbing text: removing usernames, URLs, special characters and ensuring all text is lowercase\n",
    "tweet_column = df['tweet'].astype(str).str.casefold()  # lowercase\n",
    "tweet_column.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4b94924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     as a woman you shouldn t complain about clean...\n",
       "1     boy dats cold tyga dwn bad for cuffin dat hoe...\n",
       "2     dawg you ever fuck a bitch and she start to c...\n",
       "3                               she look like a tranny\n",
       "4     the shit you hear about me might be true or i...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removes usernames first, urls, then any special characters\n",
    "clean_tweet = tweet_column.str.replace(r'(rt)?\\s?@\\w+:?', ' ', regex=True).str.replace(r'http.+', ' ', regex=True).str.replace(r'\\W+', ' ', regex=True)\n",
    "clean_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "94b63046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\raned\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\raned\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\raned\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\raned\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [woman, complain, cleaning, house, amp, man, a...\n",
      "1    [boy, dat, cold, tyga, dwn, bad, cuffin, dat, ...\n",
      "2    [dawg, ever, fuck, bitch, start, cry, confused...\n",
      "3                                 [look, like, tranny]\n",
      "4    [shit, hear, might, true, might, faker, bitch,...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#tokenization, stop words, and lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "#nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) #stopwords\n",
    "\n",
    "def clean_tokenize(text): \n",
    "    tokens = word_tokenize(text)  # Keeps contractions like \"don't\"; tokenization\n",
    "    tokens = [t.lower() for t in tokens if t.isalpha() or \"'\" in t]  # keep letters + contractions\n",
    "    tokens = [t for t in tokens if t != \"rt\" and t not in stop_words]  # remove 'rt' and stopwords\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens] #lemmatization\n",
    "    return lemmatized\n",
    "\n",
    "cleaned_tokens = clean_tweet.apply(clean_tokenize)\n",
    "print(cleaned_tokens.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98a7c355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaaaaaaaand' 'aaahhhhh' 'aahahah' 'aaliyah' 'aan' 'aap' 'aaron'\n",
      " 'aaronmacgruder' 'aaryn' 'ab' 'abandonado' 'abbey' 'abby' 'abc' 'abdelka'\n",
      " 'abduction' 'abdullah' 'abdurahman' 'abed' 'abel' 'aberdeen' 'ability'\n",
      " 'able' 'abo' 'aborted' 'abortion' 'abou' 'abound' 'abouta' 'abouttime'\n",
      " 'abraham' 'absent' 'absolute' 'absolutely' 'absoluteyvile' 'absolved'\n",
      " 'abstract' 'absurd' 'abt' 'abu' 'abundance' 'abus' 'abuse' 'abused'\n",
      " 'abuser' 'abusive' 'ac' 'aca' 'acab' 'academic' 'accelerated' 'accent'\n",
      " 'accept' 'acceptable' 'acceptance' 'accepted' 'access' 'accessible'\n",
      " 'accessorize' 'accessory' 'accident' 'accidentally' 'accipiter'\n",
      " 'accipitridae' 'accnt' 'accolade' 'accompanied' 'accord' 'according'\n",
      " 'accordingly' 'account' 'accountable' 'accountant' 'acct' 'accuracy'\n",
      " 'accurate' 'accurately' 'accused' 'accuses' 'accustomed' 'acdc' 'ace'\n",
      " 'aceptar' 'aceves' 'ach' 'achieve' 'achilles' 'aching' 'acid' 'ackin'\n",
      " 'acknowledge' 'acknowledged' 'acknowledging' 'acl' 'acne' 'acoustic'\n",
      " 'acquire' 'acre' 'acronym']\n",
      "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0               0      3            0                   0        3      2   \n",
      "1               1      3            0                   3        0      1   \n",
      "2               2      3            0                   3        0      1   \n",
      "3               3      3            0                   2        1      1   \n",
      "4               4      6            0                   6        0      1   \n",
      "...           ...    ...          ...                 ...      ...    ...   \n",
      "24778       25291      3            0                   2        1      1   \n",
      "24779       25292      3            0                   1        2      2   \n",
      "24780       25294      3            0                   3        0      1   \n",
      "24781       25295      6            0                   6        0      1   \n",
      "24782       25296      3            0                   0        3      2   \n",
      "\n",
      "                                                   tweet  \\\n",
      "0      !!! RT @mayasolovely: As a woman you shouldn't...   \n",
      "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
      "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
      "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
      "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
      "...                                                  ...   \n",
      "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...   \n",
      "24779  you've gone and broke the wrong heart baby, an...   \n",
      "24780  young buck wanna eat!!.. dat nigguh like I ain...   \n",
      "24781              youu got wild bitches tellin you lies   \n",
      "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...   \n",
      "\n",
      "                                            cleaned_text  \n",
      "0      woman complain cleaning house amp man always t...  \n",
      "1         boy dat cold tyga dwn bad cuffin dat hoe place  \n",
      "2           dawg ever fuck bitch start cry confused shit  \n",
      "3                                       look like tranny  \n",
      "4         shit hear might true might faker bitch told ya  \n",
      "...                                                  ...  \n",
      "24778  muthaf lie right tl trash mine bible scripture...  \n",
      "24779    gone broke wrong heart baby drove redneck crazy  \n",
      "24780  young buck wan na eat dat nigguh like aint fuc...  \n",
      "24781                     youu got wild bitch tellin lie  \n",
      "24782  ruffled ntac eileen dahlia beautiful color com...  \n",
      "\n",
      "[24783 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df1['cleaned_text'] = cleaned_tokens.apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df1['cleaned_text'])\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(feature_names[:100])  \n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "906bbc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (27000, 32989)\n",
      "['aa' 'aaa' 'aaaa' 'aaaaa' 'aaaaaaaaaaaaaaaaa' 'aaaaaaacopyrighta'\n",
      " 'aaaaaaareaareaaaaaaaaaaaaaa' 'aaaaarrrrrggggghhhhh' 'aaaacg'\n",
      " 'aaadonaaat' 'aaah' 'aaand' 'aachen' 'aaeyou' 'aag' 'aah' 'aaib' 'aaj'\n",
      " 'aak' 'aalukkoru' 'aand' 'aanti' 'aap' 'aardvark' 'aaron' 'aaroncrick'\n",
      " 'aarp' 'aau' 'ab' 'aba' 'aback' 'abacus' 'abandon' 'abandoned' 'abash'\n",
      " 'abated' 'abaxial' 'abb' 'abba' 'abbey' 'abbott' 'abbreviated'\n",
      " 'abbreviation' 'abc' 'abd' 'abdf' 'abdomen' 'abduce' 'abdul' 'abdullah'\n",
      " 'abe' 'abecedary' 'abeh' 'abel' 'abelson' 'aberdeen' 'abet' 'abeyance'\n",
      " 'abf' 'abhishek' 'abhorrent' 'abidance' 'abide' 'abiding' 'abigail'\n",
      " 'ability' 'abiogenic' 'abject' 'abk' 'abkhazia' 'able' 'abm' 'abnegation'\n",
      " 'abner' 'abnormal' 'abnormality' 'aboard' 'abode' 'abolish' 'abolished'\n",
      " 'abolishment' 'abolitionist' 'abominable' 'abominably' 'abominate'\n",
      " 'abomination' 'aboridzinima' 'aboriginal' 'aborigine' 'abort' 'aborted'\n",
      " 'abortion' 'abortive' 'abortively' 'abound' 'abp' 'abraham' 'abrahamic'\n",
      " 'abramowicz' 'abrams']\n"
     ]
    }
   ],
   "source": [
    "#PREPROCESSING FOR HateSpeechDatasetBalanced.csv\n",
    "\n",
    "#Load dataset and take a 27,000-row sample; easier to have random 27,000 samples due to how big the actual dataset is \n",
    "df = pd.read_csv(\"TrainingData/HateSpeechDatasetBalanced.csv\")\n",
    "df_subset = df.sample(n=27000, random_state=42).copy()\n",
    "\n",
    "df_subset['Content'] = df_subset['Content'].astype(str).str.casefold()\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    tokens = word_tokenize(text)  # splits into words and keeps contractions\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # keep only alphabetic tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "df_subset['cleaned_tokens'] = df_subset['Content'].apply(clean_tokenize)\n",
    "df_subset['cleaned_text'] = df_subset['cleaned_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "df_subset['cleaned_text'] = df_subset['cleaned_text'].fillna('')\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df_subset['cleaned_text'])  # Features\n",
    "y = df_subset['Label']  # Target labels\n",
    "\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "print(tfidf.get_feature_names_out()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# since both datasets have \"Content/Tweet\" and \"Label/class\" columns, the following code will merge the two datasets into one dataframe while maintaining balance\n",
    "\n",
    "#============================================ labeled_data.csv ============================================\n",
    "df1 = pd.read_csv(\"TrainingData/labeled_data.csv\")\n",
    "#reduced_df1['class'].value_counts()    #1430 tweets marked at hate-speech, so we will extract 1430 marked for hate speech and 1430 that aren't\n",
    "\n",
    "# get rid of all the extra columns that aren't relevant\n",
    "reduced_df1 = df1[['tweet','class']]\n",
    "\n",
    "# extracts 1430 marked for hate speech and 1430 that aren't and combine into one df\n",
    "hatespeech = reduced_df1[reduced_df1['class']==0].sample(n=1430, random_state=42).copy() # hate speech\n",
    "nonHateful = reduced_df1[reduced_df1['class']==2].sample(n=1430, random_state=42).copy() # not hate speech\n",
    "sampled_hatespeech_df = pd.concat([hatespeech,nonHateful])\n",
    "\n",
    "# edit the values in 'Class' so that they match the values for HateSpeechDatasetBalanced.csv \n",
    "# Clean: 0, Hate speech: 1\n",
    "sampled_hatespeech_df['Content'] = sampled_hatespeech_df['tweet']\n",
    "sampled_hatespeech_df['Label'] = sampled_hatespeech_df['class'].replace(to_replace=[0,2], value = [1,0])\n",
    "sampled_hatespeech_df = sampled_hatespeech_df.drop(columns=['tweet','class'])\n",
    "\n",
    "\n",
    "#============================================ HateSpeechDatasetBalanced.csv ============================================\n",
    "df = pd.read_csv(\"TrainingData/HateSpeechDatasetBalanced.csv\")\n",
    "df_subset = df.sample(n=27000, random_state=42).copy()\n",
    "\n",
    "temp_df = df_subset[['Content','Label']]\n",
    "\n",
    "# combine both datasets into one:\n",
    "training_data_df = pd.concat([sampled_hatespeech_df,temp_df])\n",
    "\n",
    "\n",
    "\n",
    "training_data_df['Content']= training_data_df['Content'].astype(str).str.casefold()\n",
    "clean_content = training_data_df['Content'].str.replace(r'(rt)?\\s?@\\w+:?', ' ', regex=True).str.replace(r'http.+', ' ', regex=True)\n",
    "training_data_df['Content'] = clean_content\n",
    "\n",
    "training_data_df.to_csv('combined_dataset3.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3499a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanContent</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>i do not like talking to you faggot and i did ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19697</th>\n",
       "      <td>what straight guys take a picture of themselve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749</th>\n",
       "      <td>america today the rule of thumb is when in dou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4615</th>\n",
       "      <td>you tell me coon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15140</th>\n",
       "      <td>this nigguh chris paul</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            CleanContent  Label\n",
       "2499   i do not like talking to you faggot and i did ...      1\n",
       "19697  what straight guys take a picture of themselve...      1\n",
       "5749   america today the rule of thumb is when in dou...      1\n",
       "4615                                    you tell me coon      1\n",
       "15140                             this nigguh chris paul      1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Clean tweets to support emoji detection\n",
    "# ---------------------------------------------------------\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Define emoji-aware cleaning function\n",
    "def clean_tweet(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove 'rt', mentions, and links\n",
    "    text = re.sub(r'\\brt\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'@\\w+:?', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Convert emojis to descriptive words\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    \n",
    "    # Remove HTML entities (like &#8230;)\n",
    "    text = re.sub(r'&#\\d+;', '', text)\n",
    "    \n",
    "    # Remove special characters (keep only alphanumerics and emoji words)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Normalize spacing + lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the function to create a cleaned version of the content\n",
    "training_data_df['CleanContent'] = training_data_df['Content'].apply(clean_tweet)\n",
    "\n",
    "# Save to a new emoji-aware dataset\n",
    "training_data_df[['CleanContent', 'Label']].to_csv(\"combined_dataset_emoji_cleaned.csv\", index=False)\n",
    "\n",
    "# Preview\n",
    "training_data_df[['CleanContent', 'Label']].head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
