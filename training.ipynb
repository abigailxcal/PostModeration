{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ec761811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "\n",
    "# save for later\n",
    "#from sklearn.decomposition import PCA\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.datasets import make_classification\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import classification_report\n",
    "\n",
    "#os.chdir(r\"C:\\Users\\raned\\Documents\\GitHub\\PostModeration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29653db5",
   "metadata": {},
   "source": [
    "This notebook will start off with preprocessing the two csv files to train different supervised learning models. \n",
    "- Removal of usernames, URLs, and special characters\n",
    "- Lowercasing text\n",
    "- Tokenization (nltk or spaCy): breaking text into smaller units \n",
    "- Stopword removal: remove common words that become index terms (\"and\", \"or\", \"the\", \"in\")\n",
    "- Lemmatization: reduces words to their base or dictionary form\n",
    "- TF-IDF vectorization for feature extraction: a technique that converts text data into numerical vectors, representing the importance of words in a document relative to a collection of documents, by combining term frequency with inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a025d07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0         count   hate_speech  offensive_language  \\\n",
      "count  24783.000000  24783.000000  24783.000000        24783.000000   \n",
      "mean   12681.192027      3.243473      0.280515            2.413711   \n",
      "std     7299.553863      0.883060      0.631851            1.399459   \n",
      "min        0.000000      3.000000      0.000000            0.000000   \n",
      "25%     6372.500000      3.000000      0.000000            2.000000   \n",
      "50%    12703.000000      3.000000      0.000000            3.000000   \n",
      "75%    18995.500000      3.000000      0.000000            3.000000   \n",
      "max    25296.000000      9.000000      7.000000            9.000000   \n",
      "\n",
      "            neither         class  \n",
      "count  24783.000000  24783.000000  \n",
      "mean       0.549247      1.110277  \n",
      "std        1.113299      0.462089  \n",
      "min        0.000000      0.000000  \n",
      "25%        0.000000      1.000000  \n",
      "50%        0.000000      1.000000  \n",
      "75%        0.000000      1.000000  \n",
      "max        9.000000      2.000000  \n",
      "(24783, 7)\n",
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24783 entries, 0 to 24782\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Unnamed: 0          24783 non-null  int64 \n",
      " 1   count               24783 non-null  int64 \n",
      " 2   hate_speech         24783 non-null  int64 \n",
      " 3   offensive_language  24783 non-null  int64 \n",
      " 4   neither             24783 non-null  int64 \n",
      " 5   class               24783 non-null  int64 \n",
      " 6   tweet               24783 non-null  object\n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"TrainingData/labeled_data.csv\")\n",
    "print(df.describe())\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112d42c",
   "metadata": {},
   "source": [
    "**count**: number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were\n",
    "\n",
    "**hate_speech**: number of CF users who judged the tweet to be hate speech\n",
    "\n",
    "**offensive_language**: number of CF users who judged the tweet to be offensive\n",
    "\n",
    "**neither**: number of CF users who judged the tweet to be neither offensive nor non-offensive\n",
    "\n",
    "**class**: class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7a023f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    !!! rt @mayasolovely: as a woman you shouldn't...\n",
       "1    !!!!! rt @mleew17: boy dats cold...tyga dwn ba...\n",
       "2    !!!!!!! rt @urkindofbrand dawg!!!! rt @80sbaby...\n",
       "3    !!!!!!!!! rt @c_g_anderson: @viva_based she lo...\n",
       "4    !!!!!!!!!!!!! rt @shenikaroberts: the shit you...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrubbing text: removing usernames, URLs, special characters and ensuring all text is lowercase\n",
    "tweet_column = df['tweet'].astype(str).str.casefold()  # lowercase\n",
    "tweet_column.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4b94924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     as a woman you shouldn t complain about clean...\n",
       "1     boy dats cold tyga dwn bad for cuffin dat hoe...\n",
       "2     dawg you ever fuck a bitch and she start to c...\n",
       "3                               she look like a tranny\n",
       "4     the shit you hear about me might be true or i...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removes usernames first, urls, then any special characters\n",
    "clean_tweet = tweet_column.str.replace(r'(rt)?\\s?@\\w+:?', ' ', regex=True).str.replace(r'http.+', ' ', regex=True).str.replace(r'\\W+', ' ', regex=True)\n",
    "clean_tweet.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b63046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [woman, complain, cleaning, house, amp, man, a...\n",
      "1    [boy, dat, cold, tyga, dwn, bad, cuffin, dat, ...\n",
      "2    [dawg, ever, fuck, bitch, start, cry, confused...\n",
      "3                                 [look, like, tranny]\n",
      "4    [shit, hear, might, true, might, faker, bitch,...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#tokenization, stop words, and lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "#nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) #stopwords\n",
    "\n",
    "def clean_tokenize(text): \n",
    "    tokens = word_tokenize(text)  # Keeps contractions like \"don't\"; tokenization\n",
    "    tokens = [t.lower() for t in tokens if t.isalpha() or \"'\" in t]  # keep letters + contractions\n",
    "    tokens = [t for t in tokens if t != \"rt\" and t not in stop_words]  # remove 'rt' and stopwords\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens] #lemmatization\n",
    "    return lemmatized\n",
    "\n",
    "cleaned_tokens = clean_tweet.apply(clean_tokenize)\n",
    "print(cleaned_tokens.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eab4ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaaaaaaaand' 'aaahhhhh' 'aahahah' 'aaliyah' 'aan' 'aap' 'aaron'\n",
      " 'aaronmacgruder' 'aaryn' 'ab' 'abandonado' 'abbey' 'abby' 'abc' 'abdelka'\n",
      " 'abduction' 'abdullah' 'abdurahman' 'abed' 'abel' 'aberdeen' 'ability'\n",
      " 'able' 'abo' 'aborted' 'abortion' 'abou' 'abound' 'abouta' 'abouttime'\n",
      " 'abraham' 'absent' 'absolute' 'absolutely' 'absoluteyvile' 'absolved'\n",
      " 'abstract' 'absurd' 'abt' 'abu' 'abundance' 'abus' 'abuse' 'abused'\n",
      " 'abuser' 'abusive' 'ac' 'aca' 'acab' 'academic' 'accelerated' 'accent'\n",
      " 'accept' 'acceptable' 'acceptance' 'accepted' 'access' 'accessible'\n",
      " 'accessorize' 'accessory' 'accident' 'accidentally' 'accipiter'\n",
      " 'accipitridae' 'accnt' 'accolade' 'accompanied' 'accord' 'according'\n",
      " 'accordingly' 'account' 'accountable' 'accountant' 'acct' 'accuracy'\n",
      " 'accurate' 'accurately' 'accused' 'accuses' 'accustomed' 'acdc' 'ace'\n",
      " 'aceptar' 'aceves' 'ach' 'achieve' 'achilles' 'aching' 'acid' 'ackin'\n",
      " 'acknowledge' 'acknowledged' 'acknowledging' 'acl' 'acne' 'acoustic'\n",
      " 'acquire' 'acre' 'acronym']\n",
      "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0               0      3            0                   0        3      2   \n",
      "1               1      3            0                   3        0      1   \n",
      "2               2      3            0                   3        0      1   \n",
      "3               3      3            0                   2        1      1   \n",
      "4               4      6            0                   6        0      1   \n",
      "...           ...    ...          ...                 ...      ...    ...   \n",
      "24778       25291      3            0                   2        1      1   \n",
      "24779       25292      3            0                   1        2      2   \n",
      "24780       25294      3            0                   3        0      1   \n",
      "24781       25295      6            0                   6        0      1   \n",
      "24782       25296      3            0                   0        3      2   \n",
      "\n",
      "                                                   tweet  \\\n",
      "0      !!! RT @mayasolovely: As a woman you shouldn't...   \n",
      "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
      "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
      "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
      "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
      "...                                                  ...   \n",
      "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...   \n",
      "24779  you've gone and broke the wrong heart baby, an...   \n",
      "24780  young buck wanna eat!!.. dat nigguh like I ain...   \n",
      "24781              youu got wild bitches tellin you lies   \n",
      "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...   \n",
      "\n",
      "                                            cleaned_text  \n",
      "0      woman complain cleaning house amp man always t...  \n",
      "1         boy dat cold tyga dwn bad cuffin dat hoe place  \n",
      "2           dawg ever fuck bitch start cry confused shit  \n",
      "3                                       look like tranny  \n",
      "4         shit hear might true might faker bitch told ya  \n",
      "...                                                  ...  \n",
      "24778  muthaf lie right tl trash mine bible scripture...  \n",
      "24779    gone broke wrong heart baby drove redneck crazy  \n",
      "24780  young buck wan na eat dat nigguh like aint fuc...  \n",
      "24781                     youu got wild bitch tellin lie  \n",
      "24782  ruffled ntac eileen dahlia beautiful color com...  \n",
      "\n",
      "[24783 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df1['cleaned_text'] = cleaned_tokens.apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df1['cleaned_text'])\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(feature_names[:100])  \n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0d630e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (27000, 33004)\n",
      "['aa' 'aaa' 'aaaa' 'aaaaa' 'aaaaaaaaaaaaaaaaa' 'aaaaaaacopyrighta'\n",
      " 'aaaaaaareaareaaaaaaaaaaaaaa' 'aaaaarrrrrggggghhhhh' 'aaaacg'\n",
      " 'aaadonaaat' 'aaah' 'aaand' 'aachen' 'aaeyou' 'aag' 'aah' 'aaib' 'aaj'\n",
      " 'aak' 'aalukkoru' 'aand' 'aanti' 'aap' 'aardvark' 'aaron' 'aaroncrick'\n",
      " 'aarp' 'aau' 'ab' 'aba' 'aback' 'abacus' 'abandon' 'abandoned' 'abash'\n",
      " 'abated' 'abaxial' 'abb' 'abba' 'abbey' 'abbott' 'abbreviated'\n",
      " 'abbreviation' 'abc' 'abd' 'abdf' 'abdomen' 'abduce' 'abdul' 'abdullah'\n",
      " 'abe' 'abecedary' 'abeh' 'abel' 'abelson' 'aberdeen' 'abet' 'abeyance'\n",
      " 'abf' 'abhishek' 'abhorrent' 'abidance' 'abide' 'abiding' 'abigail'\n",
      " 'ability' 'abiogenic' 'abject' 'abk' 'abkhazia' 'able' 'abm' 'abnegation'\n",
      " 'abner' 'abnormal' 'abnormality' 'aboard' 'abode' 'abolish' 'abolished'\n",
      " 'abolishment' 'abolitionist' 'abominable' 'abominably' 'abominate'\n",
      " 'abomination' 'aboridzinima' 'aboriginal' 'aborigine' 'abort' 'aborted'\n",
      " 'abortion' 'abortive' 'abortively' 'abound' 'abp' 'abraham' 'abrahamic'\n",
      " 'abramowicz' 'abrams']\n"
     ]
    }
   ],
   "source": [
    "#PREPROCESSING FOR HateSpeechDatasetBalanced.csv\n",
    "\n",
    "#Load dataset and take a 27,000-row sample; easier to have random 27,000 samples due to how big the actual dataset is \n",
    "df = pd.read_csv(\"TrainingData/HateSpeechDatasetBalanced.csv\")\n",
    "df_subset = df.sample(n=27000, random_state=42).copy()\n",
    "\n",
    "df_subset['Content'] = df_subset['Content'].astype(str).str.casefold()\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    tokens = word_tokenize(text)  # splits into words and keeps contractions\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # keep only alphabetic tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "df_subset['cleaned_tokens'] = df_subset['Content'].apply(clean_tokenize)\n",
    "df_subset['cleaned_text'] = df_subset['cleaned_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "df_subset['cleaned_text'] = df_subset['cleaned_text'].fillna('')\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df_subset['cleaned_text'])  # Features\n",
    "y = df_subset['Label']  # Target labels\n",
    "\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "print(tfidf.get_feature_names_out()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1547d2",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae742d90",
   "metadata": {},
   "source": [
    "## Start Here\n",
    "The code below combines the two datasets into one dataframe before any thing gets preprocessed. Since SVMs and LSTMs require different formats for their text input data, I created new columns 'tokenized_clean_text' and 'cleaned_text' that contain different formats of text so that training_data_df['Content'] doesn't have to be re-preprocessed all over again wheneveer we switch between models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "296ff555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# since both datasets have \"Content/Tweet\" and \"Label/class\" columns, the following code will merge the two datasets into one dataframe while maintaining balance\n",
    "\n",
    "#============================================ labeled_data.csv ============================================\n",
    "df1 = pd.read_csv(\"TrainingData/labeled_data.csv\")\n",
    "#reduced_df1['class'].value_counts()    #1430 tweets marked at hate-speech, so we will extract 1430 marked for hate speech and 1430 that aren't\n",
    "\n",
    "# get rid of all the extra columns that aren't relevant\n",
    "reduced_df1 = df1[['tweet','class']]\n",
    "\n",
    "# extracts 1430 marked for hate speech and 1430 that aren't and combine into one df\n",
    "hatespeech = reduced_df1[reduced_df1['class']==0].sample(n=1430, random_state=42).copy() # hate speech\n",
    "nonHateful = reduced_df1[reduced_df1['class']==2].sample(n=1430, random_state=42).copy() # not hate speech\n",
    "sampled_hatespeech_df = pd.concat([hatespeech,nonHateful])\n",
    "\n",
    "# edit the values in 'Class' so that they match the values for HateSpeechDatasetBalanced.csv \n",
    "# Clean: 0, Hate speech: 1\n",
    "sampled_hatespeech_df['Content'] = sampled_hatespeech_df['tweet']\n",
    "sampled_hatespeech_df['Label'] = sampled_hatespeech_df['class'].replace(to_replace=[0,2], value = [1,0])\n",
    "sampled_hatespeech_df = sampled_hatespeech_df.drop(columns=['tweet','class'])\n",
    "\n",
    "\n",
    "#============================================ HateSpeechDatasetBalanced.csv ============================================\n",
    "df = pd.read_csv(\"TrainingData/HateSpeechDatasetBalanced.csv\")\n",
    "df_subset = df.sample(n=27000, random_state=42).copy()\n",
    "\n",
    "temp_df = df_subset[['Content','Label']]\n",
    "\n",
    "# combine both datasets into one:\n",
    "training_data_df = pd.concat([sampled_hatespeech_df,temp_df])\n",
    "\n",
    "\n",
    "\n",
    "training_data_df['Content']= training_data_df['Content'].astype(str).str.casefold()\n",
    "clean_content = training_data_df['Content'].str.replace(r'(rt)?\\s?@\\w+:?', ' ', regex=True).str.replace(r'http.+', ' ', regex=True)\n",
    "training_data_df['Content'] = clean_content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "819254b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2d72f27c-2057-42a1-8501-c92c0e07bf86",
       "rows": [
        [
         "2499",
         "  i do not like talking to you faggot and i did but in a nicely way fag",
         "1"
        ],
        [
         "19697",
         "  what straight guys take a picture of themselves naked in a hot tun.what fags   &#8230;",
         "1"
        ],
        [
         "5749",
         "   america today, the rule of thumb is: when in doubt, blame \"whitey\"",
         "1"
        ],
        [
         "4615",
         "  you tell me, coon.",
         "1"
        ],
        [
         "15140",
         "  this nigguh chris paul",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>i do not like talking to you faggot and i di...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19697</th>\n",
       "      <td>what straight guys take a picture of themsel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749</th>\n",
       "      <td>america today, the rule of thumb is: when i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4615</th>\n",
       "      <td>you tell me, coon.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15140</th>\n",
       "      <td>this nigguh chris paul</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Content  Label\n",
       "2499     i do not like talking to you faggot and i di...      1\n",
       "19697    what straight guys take a picture of themsel...      1\n",
       "5749      america today, the rule of thumb is: when i...      1\n",
       "4615                                  you tell me, coon.      1\n",
       "15140                             this nigguh chris paul      1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b236b",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6332e4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Content  Label  \\\n",
      "2499     i do not like talking to you faggot and i di...      1   \n",
      "19697    what straight guys take a picture of themsel...      1   \n",
      "5749      america today, the rule of thumb is: when i...      1   \n",
      "4615                                  you tell me, coon.      1   \n",
      "15140                             this nigguh chris paul      1   \n",
      "\n",
      "                                          cleaned_tokens  \\\n",
      "2499           [like, talking, faggot, nicely, way, fag]   \n",
      "19697    [straight, guy, take, picture, naked, hot, fag]   \n",
      "5749   [america, today, rule, thumb, doubt, blame, wh...   \n",
      "4615                                        [tell, coon]   \n",
      "15140                              [nigguh, chris, paul]   \n",
      "\n",
      "                            tokenized_cleaned_text  \n",
      "2499            like talking faggot nicely way fag  \n",
      "19697      straight guy take picture naked hot fag  \n",
      "5749   america today rule thumb doubt blame whitey  \n",
      "4615                                     tell coon  \n",
      "15140                            nigguh chris paul  \n",
      "TF-IDF shape: (29860, 813893)\n",
      "['aa' 'aa aaa' 'aa aaa aaa' 'aa armament' 'aa armament admittedly'\n",
      " 'aa better' 'aa better omit' 'aa bfmv' 'aa bfmv fob' 'aa blink'\n",
      " 'aa blink tdg' 'aa bunker' 'aa bunker tank' 'aa car' 'aa car edit'\n",
      " 'aa community' 'aa community looking' 'aa could' 'aa could stop' 'aa gac'\n",
      " 'aa lol' 'aa mason' 'aa mason love' 'aa miw' 'aa miw btf' 'aa page'\n",
      " 'aa page manually' 'aa shit' 'aa shit shit' 'aa smug' 'aa smug face'\n",
      " 'aa udtapunjab' 'aaa' 'aaa aa' 'aaa aa aaa' 'aaa aaa' 'aaa aaa areaa'\n",
      " 'aaa areaa' 'aaa areaa aa' 'aaa know' 'aaa know ainaaat' 'aaa mean'\n",
      " 'aaa mean ole' 'aaa padding' 'aaa padding em' 'aaa slut'\n",
      " 'aaa slut waiting' 'aaa woo' 'aaa woo dong' 'aaaa' 'aaaaa' 'aaaaa ah'\n",
      " 'aaaaa ah aaaaa' 'aaaaaaaaaaaaaaaaa' 'aaaaaaaaaaaaaaaaa obavezno'\n",
      " 'aaaaaaaaaaaaaaaaa obavezno pored' 'aaaaaaaaand' 'aaaaaaaaand begin'\n",
      " 'aaaaaaaaand begin fucking' 'aaaaaaacopyrighta'\n",
      " 'aaaaaaareaareaaaaaaaaaaaaaa' 'aaaaaaareaareaaaaaaaaaaaaaa life'\n",
      " 'aaaaaaareaareaaaaaaaaaaaaaa life usa' 'aaaaarrrrrggggghhhhh'\n",
      " 'aaaaarrrrrggggghhhhh muellerreport'\n",
      " 'aaaaarrrrrggggghhhhh muellerreport even' 'aaaacg' 'aaadonaaat'\n",
      " 'aaadonaaat let' 'aaadonaaat let think' 'aaah' 'aaah gladstone'\n",
      " 'aaah gladstone fascinating' 'aaand' 'aaand call' 'aaand call maine'\n",
      " 'aachen' 'aachen daphne' 'aachen daphne wie' 'aachen hello'\n",
      " 'aachen hello wondering' 'aachen moving' 'aachen moving cite'\n",
      " 'aachen translation' 'aaeyou' 'aaeyou arrogant' 'aaeyou arrogant ye'\n",
      " 'aag' 'aag leigh' 'aag leigh chatty' 'aah' 'aah aah' 'aah aah first'\n",
      " 'aah first' 'aah first grouping' 'aaib' 'aaib set' 'aaib set syria' 'aaj'\n",
      " 'aaj taxon']\n"
     ]
    }
   ],
   "source": [
    "# Cleaning for models that require sparse vectors as text  (SVMs,Logistic Regression, etc)\n",
    "\n",
    "#tokenization, stop words, and lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "#nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) #stopwords\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    tokens = word_tokenize(text)  # splits into words and keeps contractions\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # keep only alphabetic tokens\n",
    "    tokens = [t for t in tokens if t != \"rt\" and t not in stop_words]  # remove 'rt' and stopwords\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "training_data_df['cleaned_tokens'] = training_data_df['Content'].apply(clean_tokenize)\n",
    "training_data_df['tokenized_cleaned_text'] = training_data_df['cleaned_tokens'].apply(lambda x: ' '.join(x))\n",
    "training_data_df['tokenized_cleaned_text'] = training_data_df['tokenized_cleaned_text'].fillna('')\n",
    "\n",
    "print(training_data_df.head())\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3))      # playing around with ngram_range, might go up to 3\n",
    "\n",
    "X = tfidf.fit_transform(training_data_df['tokenized_cleaned_text'])  # Features\n",
    "y = training_data_df['Label']  # Target labels\n",
    "\n",
    "print(\"TF-IDF shape:\", X.shape)\n",
    "print(tfidf.get_feature_names_out()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bd28bb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2305  604]\n",
      " [ 555 2508]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80      2909\n",
      "           1       0.81      0.82      0.81      3063\n",
      "\n",
      "    accuracy                           0.81      5972\n",
      "   macro avg       0.81      0.81      0.81      5972\n",
      "weighted avg       0.81      0.81      0.81      5972\n",
      "\n",
      "The tweet: ' stfu you piece of trash ' has been flagged for hate speech. . Value:  1\n",
      "The tweet: ' Hope you're having a good day you cute piece of shit ' is permitted . Value:  0\n",
      "The tweet: ' hello there ' is permitted . Value:  0\n",
      "The tweet: ' today was fine ' is permitted . Value:  0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC  # for text, linear SVM usually works best\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LinearSVC(C=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "legend = {0: \"' is permitted\", 1: \"' has been flagged for hate speech.\"}\n",
    "\n",
    "\n",
    "def moderate(tweet):\n",
    "    tweet_clean = \" \".join(clean_tokenize(tweet))\n",
    "    tweet_vec = tfidf.transform([tweet_clean])\n",
    "    pred = model.predict(tweet_vec)[0]\n",
    "    print(\"The tweet: '\",tweet,legend[pred],\". Value: \",pred)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "moderate(\"stfu you piece of trash\")  # 1\n",
    "moderate(\"Hope you're having a good day you cute piece of shit\")  # 0\n",
    "moderate(\"hello there\")\n",
    "moderate(\"today was fine\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "44607cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'C': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparamter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(LinearSVC(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f4221a",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cc8ac595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cleaned_tokens",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "tokenized_cleaned_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cleaned_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "877cb128-ae60-46cf-b71e-476aa484ba04",
       "rows": [
        [
         "2499",
         "@AustinG1135 I do not like talking to you faggot and I did but in a nicely way fag",
         "1",
         "['I', 'like', 'talking', 'faggot', 'I', 'nicely', 'way', 'fag']",
         "I like talking faggot I nicely way fag",
         "i do not like talking to you faggot and i did but in a nicely way fag"
        ],
        [
         "19697",
         "RT @mitchmancuso: @BrantPrintup:What straight guys take a picture of themselves naked in a hot tun.What fags @jakesiwy @Ryan_Murphy3 @Randy&#8230;",
         "1",
         "['RT', 'mitchmancuso', 'BrantPrintup', 'What', 'straight', 'guy', 'take', 'picture', 'naked', 'hot', 'fag', 'jakesiwy', 'Randy']",
         "RT mitchmancuso BrantPrintup What straight guy take picture naked hot fag jakesiwy Randy",
         "rtwhat straight guys take a picture of themselves naked in a hot tunwhat fags"
        ],
        [
         "5749",
         "@clinchmtn316 @sixonesixband AMERICA today, the rule of thumb is: when in doubt, blame \"whitey\"",
         "1",
         "['sixonesixband', 'AMERICA', 'today', 'rule', 'thumb', 'doubt', 'blame', 'whitey']",
         "sixonesixband AMERICA today rule thumb doubt blame whitey",
         "america today the rule of thumb is when in doubt blame whitey"
        ],
        [
         "4615",
         "@STACCS_WNT_FOLD you tell me, coon.",
         "1",
         "['tell', 'coon']",
         "tell coon",
         "you tell me coon"
        ],
        [
         "15140",
         "RT @FAAMMoverALL: This nigguh Chris Paul",
         "1",
         "['RT', 'FAAMMoverALL', 'This', 'nigguh', 'Chris', 'Paul']",
         "RT FAAMMoverALL This nigguh Chris Paul",
         "rt this nigguh chris paul"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "      <th>tokenized_cleaned_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>@AustinG1135 I do not like talking to you fagg...</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, like, talking, faggot, I, nicely, way, fag]</td>\n",
       "      <td>I like talking faggot I nicely way fag</td>\n",
       "      <td>i do not like talking to you faggot and i did ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19697</th>\n",
       "      <td>RT @mitchmancuso: @BrantPrintup:What straight ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[RT, mitchmancuso, BrantPrintup, What, straigh...</td>\n",
       "      <td>RT mitchmancuso BrantPrintup What straight guy...</td>\n",
       "      <td>rtwhat straight guys take a picture of themsel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749</th>\n",
       "      <td>@clinchmtn316 @sixonesixband AMERICA today, th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[sixonesixband, AMERICA, today, rule, thumb, d...</td>\n",
       "      <td>sixonesixband AMERICA today rule thumb doubt b...</td>\n",
       "      <td>america today the rule of thumb is when in dou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4615</th>\n",
       "      <td>@STACCS_WNT_FOLD you tell me, coon.</td>\n",
       "      <td>1</td>\n",
       "      <td>[tell, coon]</td>\n",
       "      <td>tell coon</td>\n",
       "      <td>you tell me coon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15140</th>\n",
       "      <td>RT @FAAMMoverALL: This nigguh Chris Paul</td>\n",
       "      <td>1</td>\n",
       "      <td>[RT, FAAMMoverALL, This, nigguh, Chris, Paul]</td>\n",
       "      <td>RT FAAMMoverALL This nigguh Chris Paul</td>\n",
       "      <td>rt this nigguh chris paul</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Content  Label  \\\n",
       "2499   @AustinG1135 I do not like talking to you fagg...      1   \n",
       "19697  RT @mitchmancuso: @BrantPrintup:What straight ...      1   \n",
       "5749   @clinchmtn316 @sixonesixband AMERICA today, th...      1   \n",
       "4615                 @STACCS_WNT_FOLD you tell me, coon.      1   \n",
       "15140           RT @FAAMMoverALL: This nigguh Chris Paul      1   \n",
       "\n",
       "                                          cleaned_tokens  \\\n",
       "2499     [I, like, talking, faggot, I, nicely, way, fag]   \n",
       "19697  [RT, mitchmancuso, BrantPrintup, What, straigh...   \n",
       "5749   [sixonesixband, AMERICA, today, rule, thumb, d...   \n",
       "4615                                        [tell, coon]   \n",
       "15140      [RT, FAAMMoverALL, This, nigguh, Chris, Paul]   \n",
       "\n",
       "                                  tokenized_cleaned_text  \\\n",
       "2499              I like talking faggot I nicely way fag   \n",
       "19697  RT mitchmancuso BrantPrintup What straight guy...   \n",
       "5749   sixonesixband AMERICA today rule thumb doubt b...   \n",
       "4615                                           tell coon   \n",
       "15140             RT FAAMMoverALL This nigguh Chris Paul   \n",
       "\n",
       "                                            cleaned_text  \n",
       "2499   i do not like talking to you faggot and i did ...  \n",
       "19697  rtwhat straight guys take a picture of themsel...  \n",
       "5749   america today the rule of thumb is when in dou...  \n",
       "4615                                    you tell me coon  \n",
       "15140                          rt this nigguh chris paul  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning data for models that require sequences of integers as text input  (LSTM/Deep Nets)\n",
    "import re\n",
    "\n",
    "def clean_for_lstm(text):\n",
    "    text = re.sub(r'(rt)?\\s?@\\w+:?', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "# clean_tweet = tweet_column.str.replace(r'(rt)?\\s?@\\w+:?', ' ', regex=True).str.replace(r'http.+', ' ', regex=True).str.replace(r'\\W+', ' ', regex=True)\n",
    "training_data_df['cleaned_text'] = training_data_df['Content'].apply(clean_for_lstm)\n",
    "training_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "835e5bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_14                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_14 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_14                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.6169 - loss: 0.6548 - val_accuracy: 0.7422 - val_loss: 0.5879\n",
      "Epoch 2/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - accuracy: 0.8186 - loss: 0.4985 - val_accuracy: 0.7890 - val_loss: 0.5251\n",
      "Epoch 3/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - accuracy: 0.8477 - loss: 0.4271 - val_accuracy: 0.7781 - val_loss: 0.5217\n",
      "Epoch 4/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - accuracy: 0.8678 - loss: 0.3755 - val_accuracy: 0.7815 - val_loss: 0.5507\n",
      "Epoch 5/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - accuracy: 0.8922 - loss: 0.3195 - val_accuracy: 0.7769 - val_loss: 0.5904\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.78      2968\n",
      "           1       0.78      0.78      0.78      3004\n",
      "\n",
      "    accuracy                           0.78      5972\n",
      "   macro avg       0.78      0.78      0.78      5972\n",
      "weighted avg       0.78      0.78      0.78      5972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras._tf_keras.keras.models import Sequential\n",
    "from keras._tf_keras.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# LSTM requires sequences of integers, NOT vectors\n",
    "texts = training_data_df['cleaned_text'].values\n",
    "labels = training_data_df['Label'].values\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, padding='post', maxlen=50)  # 50 = max length\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64, input_length=50),\n",
    "    Bidirectional(LSTM(64, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=5, # 7 epochs: 77% , 5 epochs had 77% accuracy, 3 epochs has 79%\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f4361199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hope youre having a good day you cute piece of shit\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "The tweet: \" Hope you're having a good day you cute piece of shit ' is permitted . Value:  0.23419806\n",
      "hello there\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "The tweet: \" hello there ' is permitted . Value:  0.23175243\n",
      "today was fine\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "The tweet: \" today was fine ' has been flagged for hate speech. . Value:  0.9286559\n",
      "stfu you piece of trash\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "The tweet: \" stfu you piece of trash ' has been flagged for hate speech. . Value:  0.9852149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def moderate_lstm(tweet):\n",
    "    cleaned = clean_for_lstm(tweet)\n",
    "    print(cleaned)\n",
    "    seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    padded = pad_sequences(seq, padding='post', maxlen=50)\n",
    "    pred = model.predict(padded)[0][0]\n",
    "    xx = lambda x : 1 if x > 0.5 else 0\n",
    "    print('The tweet: \"',tweet,legend[xx(pred)], \". Value: \",pred )\n",
    "    return 1 if pred > 0.5 else 0\n",
    "\n",
    "\n",
    "moderate_lstm(\"Hope you're having a good day you cute piece of shit\")  # 0\n",
    "moderate_lstm(\"hello there\")\n",
    "moderate_lstm(\"today was fine\")\n",
    "moderate_lstm(\"stfu you piece of trash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "028b0992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hope youre having a good day you cute piece of shit\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "The tweet: \" Hope you're having a good day you cute piece of shit ' is permitted . Value:  0.23419806\n",
      "hello there\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "The tweet: \" hello there ' is permitted . Value:  0.23175243\n",
      "today was fine\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "The tweet: \" today was fine ' has been flagged for hate speech. . Value:  0.9286559\n",
      "stfu you piece of trash\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "The tweet: \" stfu you piece of trash ' has been flagged for hate speech. . Value:  0.9852149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 64,32,4,1 and 5 epochs, batch size 32\n",
    "moderate_lstm(\"Hope you're having a good day you cute piece of shit\")  # 0\n",
    "moderate_lstm(\"hello there\")\n",
    "moderate_lstm(\"today was fine\")\n",
    "moderate_lstm(\"stfu you piece of trash\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
