{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "ec761811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "\n",
    "# save for later\n",
    "#from sklearn.decomposition import PCA\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.datasets import make_classification\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import classification_report\n",
    "\n",
    "#os.chdir(r\"C:\\Users\\raned\\Documents\\GitHub\\PostModeration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29653db5",
   "metadata": {},
   "source": [
    "This notebook will start off with preprocessing the two csv files to train different supervised learning models. \n",
    "- Removal of usernames, URLs, and special characters\n",
    "- Lowercasing text\n",
    "- Tokenization (nltk or spaCy): breaking text into smaller units \n",
    "- Stopword removal: remove common words that become index terms (\"and\", \"or\", \"the\", \"in\")\n",
    "- Lemmatization: reduces words to their base or dictionary form\n",
    "- TF-IDF vectorization for feature extraction: a technique that converts text data into numerical vectors, representing the importance of words in a document relative to a collection of documents, by combining term frequency with inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "a025d07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0         count   hate_speech  offensive_language  \\\n",
      "count  24783.000000  24783.000000  24783.000000        24783.000000   \n",
      "mean   12681.192027      3.243473      0.280515            2.413711   \n",
      "std     7299.553863      0.883060      0.631851            1.399459   \n",
      "min        0.000000      3.000000      0.000000            0.000000   \n",
      "25%     6372.500000      3.000000      0.000000            2.000000   \n",
      "50%    12703.000000      3.000000      0.000000            3.000000   \n",
      "75%    18995.500000      3.000000      0.000000            3.000000   \n",
      "max    25296.000000      9.000000      7.000000            9.000000   \n",
      "\n",
      "            neither         class  \n",
      "count  24783.000000  24783.000000  \n",
      "mean       0.549247      1.110277  \n",
      "std        1.113299      0.462089  \n",
      "min        0.000000      0.000000  \n",
      "25%        0.000000      1.000000  \n",
      "50%        0.000000      1.000000  \n",
      "75%        0.000000      1.000000  \n",
      "max        9.000000      2.000000  \n",
      "(24783, 7)\n",
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24783 entries, 0 to 24782\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Unnamed: 0          24783 non-null  int64 \n",
      " 1   count               24783 non-null  int64 \n",
      " 2   hate_speech         24783 non-null  int64 \n",
      " 3   offensive_language  24783 non-null  int64 \n",
      " 4   neither             24783 non-null  int64 \n",
      " 5   class               24783 non-null  int64 \n",
      " 6   tweet               24783 non-null  object\n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"TrainingData/labeled_data.csv\")\n",
    "print(df.describe())\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112d42c",
   "metadata": {},
   "source": [
    "**count**: number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were\n",
    "\n",
    "**hate_speech**: number of CF users who judged the tweet to be hate speech\n",
    "\n",
    "**offensive_language**: number of CF users who judged the tweet to be offensive\n",
    "\n",
    "**neither**: number of CF users who judged the tweet to be neither offensive nor non-offensive\n",
    "\n",
    "**class**: class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "1d7a023f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    !!! rt @mayasolovely: as a woman you shouldn't...\n",
       "1    !!!!! rt @mleew17: boy dats cold...tyga dwn ba...\n",
       "2    !!!!!!! rt @urkindofbrand dawg!!!! rt @80sbaby...\n",
       "3    !!!!!!!!! rt @c_g_anderson: @viva_based she lo...\n",
       "4    !!!!!!!!!!!!! rt @shenikaroberts: the shit you...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrubbing text: removing usernames, URLs, special characters and ensuring all text is lowercase\n",
    "tweet_column = df['tweet'].astype(str).str.casefold()  # lowercase\n",
    "tweet_column.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "e4b94924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     as a woman you shouldn t complain about clean...\n",
       "1     boy dats cold tyga dwn bad for cuffin dat hoe...\n",
       "2     dawg you ever fuck a bitch and she start to c...\n",
       "3                               she look like a tranny\n",
       "4     the shit you hear about me might be true or i...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removes usernames first, urls, then any special characters\n",
    "clean_tweet = tweet_column.str.replace(r'(rt)?\\s?@\\w+:?', ' ', regex=True).str.replace(r'http.+', ' ', regex=True).str.replace(r'\\W+', ' ', regex=True)\n",
    "clean_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "94b63046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/abigailcalderon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [woman, complain, cleaning, house, amp, man, a...\n",
      "1    [boy, dat, cold, tyga, dwn, bad, cuffin, dat, ...\n",
      "2    [dawg, ever, fuck, bitch, start, cry, confused...\n",
      "3                                 [look, like, tranny]\n",
      "4    [shit, hear, might, true, might, faker, bitch,...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#tokenization, stop words, and lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "#nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) #stopwords\n",
    "\n",
    "def clean_tokenize(text): \n",
    "    tokens = word_tokenize(text)  # Keeps contractions like \"don't\"; tokenization\n",
    "    tokens = [t.lower() for t in tokens if t.isalpha() or \"'\" in t]  # keep letters + contractions\n",
    "    tokens = [t for t in tokens if t != \"rt\" and t not in stop_words]  # remove 'rt' and stopwords\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens] #lemmatization\n",
    "    return lemmatized\n",
    "\n",
    "cleaned_tokens = clean_tweet.apply(clean_tokenize)\n",
    "print(cleaned_tokens.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "98a7c355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaaaaaaaand' 'aaahhhhh' 'aahahah' 'aaliyah' 'aan' 'aap' 'aaron'\n",
      " 'aaronmacgruder' 'aaryn' 'ab' 'abandonado' 'abbey' 'abby' 'abc' 'abdelka'\n",
      " 'abduction' 'abdullah' 'abdurahman' 'abed' 'abel' 'aberdeen' 'ability'\n",
      " 'able' 'abo' 'aborted' 'abortion' 'abou' 'abound' 'abouta' 'abouttime'\n",
      " 'abraham' 'absent' 'absolute' 'absolutely' 'absoluteyvile' 'absolved'\n",
      " 'abstract' 'absurd' 'abt' 'abu' 'abundance' 'abus' 'abuse' 'abused'\n",
      " 'abuser' 'abusive' 'ac' 'aca' 'acab' 'academic' 'accelerated' 'accent'\n",
      " 'accept' 'acceptable' 'acceptance' 'accepted' 'access' 'accessible'\n",
      " 'accessorize' 'accessory' 'accident' 'accidentally' 'accipiter'\n",
      " 'accipitridae' 'accnt' 'accolade' 'accompanied' 'accord' 'according'\n",
      " 'accordingly' 'account' 'accountable' 'accountant' 'acct' 'accuracy'\n",
      " 'accurate' 'accurately' 'accused' 'accuses' 'accustomed' 'acdc' 'ace'\n",
      " 'aceptar' 'aceves' 'ach' 'achieve' 'achilles' 'aching' 'acid' 'ackin'\n",
      " 'acknowledge' 'acknowledged' 'acknowledging' 'acl' 'acne' 'acoustic'\n",
      " 'acquire' 'acre' 'acronym']\n",
      "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0               0      3            0                   0        3      2   \n",
      "1               1      3            0                   3        0      1   \n",
      "2               2      3            0                   3        0      1   \n",
      "3               3      3            0                   2        1      1   \n",
      "4               4      6            0                   6        0      1   \n",
      "...           ...    ...          ...                 ...      ...    ...   \n",
      "24778       25291      3            0                   2        1      1   \n",
      "24779       25292      3            0                   1        2      2   \n",
      "24780       25294      3            0                   3        0      1   \n",
      "24781       25295      6            0                   6        0      1   \n",
      "24782       25296      3            0                   0        3      2   \n",
      "\n",
      "                                                   tweet  \\\n",
      "0      !!! RT @mayasolovely: As a woman you shouldn't...   \n",
      "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
      "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
      "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
      "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
      "...                                                  ...   \n",
      "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...   \n",
      "24779  you've gone and broke the wrong heart baby, an...   \n",
      "24780  young buck wanna eat!!.. dat nigguh like I ain...   \n",
      "24781              youu got wild bitches tellin you lies   \n",
      "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...   \n",
      "\n",
      "                                            cleaned_text  \n",
      "0      woman complain cleaning house amp man always t...  \n",
      "1         boy dat cold tyga dwn bad cuffin dat hoe place  \n",
      "2           dawg ever fuck bitch start cry confused shit  \n",
      "3                                       look like tranny  \n",
      "4         shit hear might true might faker bitch told ya  \n",
      "...                                                  ...  \n",
      "24778  muthaf lie right tl trash mine bible scripture...  \n",
      "24779    gone broke wrong heart baby drove redneck crazy  \n",
      "24780  young buck wan na eat dat nigguh like aint fuc...  \n",
      "24781                     youu got wild bitch tellin lie  \n",
      "24782  ruffled ntac eileen dahlia beautiful color com...  \n",
      "\n",
      "[24783 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df['cleaned_text'] = cleaned_tokens.apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['cleaned_text'])\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(feature_names[:100])  \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "906bbc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (27000, 33004)\n",
      "['aa' 'aaa' 'aaaa' 'aaaaa' 'aaaaaaaaaaaaaaaaa' 'aaaaaaacopyrighta'\n",
      " 'aaaaaaareaareaaaaaaaaaaaaaa' 'aaaaarrrrrggggghhhhh' 'aaaacg'\n",
      " 'aaadonaaat' 'aaah' 'aaand' 'aachen' 'aaeyou' 'aag' 'aah' 'aaib' 'aaj'\n",
      " 'aak' 'aalukkoru' 'aand' 'aanti' 'aap' 'aardvark' 'aaron' 'aaroncrick'\n",
      " 'aarp' 'aau' 'ab' 'aba' 'aback' 'abacus' 'abandon' 'abandoned' 'abash'\n",
      " 'abated' 'abaxial' 'abb' 'abba' 'abbey' 'abbott' 'abbreviated'\n",
      " 'abbreviation' 'abc' 'abd' 'abdf' 'abdomen' 'abduce' 'abdul' 'abdullah'\n",
      " 'abe' 'abecedary' 'abeh' 'abel' 'abelson' 'aberdeen' 'abet' 'abeyance'\n",
      " 'abf' 'abhishek' 'abhorrent' 'abidance' 'abide' 'abiding' 'abigail'\n",
      " 'ability' 'abiogenic' 'abject' 'abk' 'abkhazia' 'able' 'abm' 'abnegation'\n",
      " 'abner' 'abnormal' 'abnormality' 'aboard' 'abode' 'abolish' 'abolished'\n",
      " 'abolishment' 'abolitionist' 'abominable' 'abominably' 'abominate'\n",
      " 'abomination' 'aboridzinima' 'aboriginal' 'aborigine' 'abort' 'aborted'\n",
      " 'abortion' 'abortive' 'abortively' 'abound' 'abp' 'abraham' 'abrahamic'\n",
      " 'abramowicz' 'abrams']\n"
     ]
    }
   ],
   "source": [
    "#PREPROCESSING FOR HateSpeechDatasetBalanced.csv\n",
    "\n",
    "#Load dataset and take a 27,000-row sample; easier to have random 27,000 samples due to how big the actual dataset is \n",
    "df = pd.read_csv(\"TrainingData/HateSpeechDatasetBalanced.csv\")\n",
    "df_subset = df.sample(n=27000, random_state=42).copy()\n",
    "\n",
    "df_subset['Content'] = df_subset['Content'].astype(str).str.casefold()\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    tokens = word_tokenize(text)  # splits into words and keeps contractions\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # keep only alphabetic tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "df_subset['cleaned_tokens'] = df_subset['Content'].apply(clean_tokenize)\n",
    "df_subset['cleaned_text'] = df_subset['cleaned_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "df_subset['cleaned_text'] = df_subset['cleaned_text'].fillna('')\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df_subset['cleaned_text'])  # Features\n",
    "y = df_subset['Label']  # Target labels\n",
    "\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "print(tfidf.get_feature_names_out()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "b1432e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    16430\n",
      "1     4430\n",
      "Name: count, dtype: int64\n",
      "✅ Combined dataset size: (20860, 2)\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "# PARAMETERS\n",
    "emotion_data_size = 12000\n",
    "hateSpeechBalanced_size = 6000\n",
    "assert emotion_data_size + hateSpeechBalanced_size == 18000\n",
    "\n",
    "# ================== Process text.csv (emotion data) =======================\n",
    "emotions_df = pd.read_csv(\"TrainingData/text.csv\")\n",
    "emotions_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Split into 'feeling' and 'no feeling' texts\n",
    "no_feeling_size = int(emotion_data_size / 2)\n",
    "feeling_size = int(no_feeling_size / 5)\n",
    "\n",
    "feeling_df = emotions_df[emotions_df['text'].str.contains(r'\\bfeel(?:s|ing)?\\b', case=False, regex=True)]\n",
    "no_feeling_df = emotions_df[~emotions_df['text'].str.contains(r'\\bfeel(?:s|ing)?\\b', case=False, regex=True)].sample(n=no_feeling_size, random_state=42).copy()\n",
    "\n",
    "emotions = pd.DataFrame()\n",
    "for i in range(0, 5):\n",
    "    temp = feeling_df[feeling_df['label'] == i].sample(n=feeling_size, random_state=42).copy()\n",
    "    emotions = pd.concat([emotions, temp], ignore_index=True)\n",
    "\n",
    "emotions = pd.concat([emotions, no_feeling_df], ignore_index=True)\n",
    "emotions['Content'] = emotions['text']\n",
    "emotions['Label'] = 0  # all emotion data treated as clean\n",
    "emotions.drop(columns=['text', 'label'], inplace=True)\n",
    "\n",
    "# ================== Process labeled_data.csv =======================\n",
    "df1 = pd.read_csv(\"TrainingData/labeled_data.csv\")\n",
    "df1 = df1[['tweet', 'class']]\n",
    "\n",
    "hatespeech_ld = df1[df1['class'] == 0].sample(n=1430, random_state=42).copy()\n",
    "clean_ld = df1[df1['class'] == 2].sample(n=1430, random_state=42).copy()\n",
    "\n",
    "labeled_data = pd.concat([hatespeech_ld, clean_ld], ignore_index=True)\n",
    "labeled_data['Content'] = labeled_data['tweet']\n",
    "labeled_data['Label'] = labeled_data['class'].replace({0: 1, 2: 0})\n",
    "labeled_data.drop(columns=['tweet', 'class'], inplace=True)\n",
    "\n",
    "# ================== Process HateSpeechDatasetBalanced.csv =======================\n",
    "df2 = pd.read_csv(\"TrainingData/HateSpeechDatasetBalanced.csv\")\n",
    "df2 = df2[['Content', 'Label']]\n",
    "\n",
    "# Subsample from df2 to match hateSpeechBalanced_size\n",
    "hs_size = hateSpeechBalanced_size // 2\n",
    "non_hs_size = hateSpeechBalanced_size - hs_size\n",
    "\n",
    "hs_df = df2[df2['Label'] == 1].sample(n=hs_size, random_state=42).copy()\n",
    "clean_df = df2[df2['Label'] == 0].sample(n=non_hs_size, random_state=42).copy()\n",
    "balanced_df = pd.concat([hs_df, clean_df], ignore_index=True)\n",
    "\n",
    "# ================== Final Merge =======================\n",
    "# Combine emotion data, balanced hateSpeech data, and labeled_data.csv\n",
    "training_data_df = pd.concat([emotions, balanced_df, labeled_data], ignore_index=True)\n",
    "\n",
    "# ================== Cleaning =======================\n",
    "training_data_df['Content'] = training_data_df['Content'].astype(str).str.casefold()\n",
    "\n",
    "# Unescape emojis (from &#12345; to actual emoji)\n",
    "training_data_df['Content'] = training_data_df['Content'].apply(html.unescape)\n",
    "\n",
    "# Basic cleaning: remove mentions and links\n",
    "training_data_df['Content'] = training_data_df['Content'] \\\n",
    "    .str.replace(r'(rt)?\\s?@\\w+:?', ' ', regex=True) \\\n",
    "    .str.replace(r'http\\S+', ' ', regex=True)\n",
    "\n",
    "# ================== Save =======================\n",
    "training_data_df.to_csv('combined_balanced_dataset_BERT.csv', index=False) #named this after Bert Model cuz i was only training bert at the time and didnt wanna get confused when i come back\n",
    "\n",
    "# ================== Summary =======================\n",
    "print(training_data_df['Label'].value_counts())\n",
    "print(\"✅ Combined dataset size:\", training_data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc7c4c",
   "metadata": {},
   "source": [
    "IGNORE THIS PART\n",
    "\n",
    "I was trying to see if adding emoji detection would improve dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "5146548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "training_data_df\n",
    "print(hs_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "d3499a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CleanContent",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "4cbc3523-340a-40f8-a9f1-2e00100e4a7b",
       "rows": [
        [
         "0",
         "i feel nothing i feel worthless and pain and i don t know who or what i am",
         "0"
        ],
        [
         "1",
         "i instantly become terrified but it was the disregard of my decaying body that left me feeling so isolated",
         "0"
        ],
        [
         "2",
         "i want seem to be pulling me in different directions and i feel hopeless that it will work out",
         "0"
        ],
        [
         "3",
         "i write this short post for any and all of you who might be feeling disheartened by the news tonight",
         "0"
        ],
        [
         "4",
         "i don t have any money for beauty supplies or anything so i m basically stuck feeling ugly",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanContent</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel nothing i feel worthless and pain and i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i instantly become terrified but it was the di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i want seem to be pulling me in different dire...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i write this short post for any and all of you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i don t have any money for beauty supplies or ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        CleanContent  Label\n",
       "0  i feel nothing i feel worthless and pain and i...      0\n",
       "1  i instantly become terrified but it was the di...      0\n",
       "2  i want seem to be pulling me in different dire...      0\n",
       "3  i write this short post for any and all of you...      0\n",
       "4  i don t have any money for beauty supplies or ...      0"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Clean tweets to support emoji detection\n",
    "# ---------------------------------------------------------\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Define emoji-aware cleaning function\n",
    "def clean_tweet(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove 'rt', mentions, and links\n",
    "    text = re.sub(r'\\brt\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'@\\w+:?', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Convert emojis to descriptive words\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    \n",
    "    # Remove HTML entities (like &#8230;)\n",
    "    text = re.sub(r'&#\\d+;', '', text)\n",
    "    \n",
    "    # Remove special characters (keep only alphanumerics and emoji words)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Normalize spacing + lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the function to create a cleaned version of the content\n",
    "training_data_df['CleanContent'] = training_data_df['Content'].apply(clean_tweet)\n",
    "\n",
    "# Save to a new emoji-aware dataset\n",
    "training_data_df[['CleanContent', 'Label']].to_csv(\"combined_dataset_emoji_cleaned.csv\", index=False)\n",
    "\n",
    "# Preview\n",
    "training_data_df[['CleanContent', 'Label']].head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
